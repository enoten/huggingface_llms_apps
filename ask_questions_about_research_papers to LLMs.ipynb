{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5065cbca-a35b-4028-85f6-923d81f347ca",
   "metadata": {},
   "source": [
    "<img src=\"ask questions to LLMs.jpg\" width=70%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0c4a91-3191-4477-b858-aac9213f9fcf",
   "metadata": {},
   "source": [
    "## 1. Read PDF files with research papers using Llama-Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d96e764e-ba80-4ba2-8b4d-5b54463e29b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f578bb7-7f8e-48ab-ac8e-af26a16aaf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1c69ab-b754-4d7d-8c21-bc39ec737fe7",
   "metadata": {},
   "source": [
    "<b>1.1 Setup model for embeddings</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c478cf42-6550-4cd3-8a3c-c42f24dfc451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    }
   ],
   "source": [
    "# import any embedding model on HF hub (https://huggingface.co/spaces/mteb/leaderboard)\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "# Settings.embed_model = HuggingFaceEmbedding(model_name=\"thenlper/gte-large\") # alternative model\n",
    "\n",
    "Settings.llm = None\n",
    "Settings.chunk_size = 128\n",
    "Settings.chunk_overlap = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b37a1e-4cbd-49c9-b130-8b9b14d1aa4b",
   "metadata": {},
   "source": [
    "<b>1.2 Read files one by one from the directory</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6ba0322-49db-4c51-b1e6-29d80e890958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fs': <fsspec.implementations.local.LocalFileSystem at 0x16d562fce20>,\n",
       " 'errors': 'ignore',\n",
       " 'encoding': 'utf-8',\n",
       " 'exclude': None,\n",
       " 'recursive': True,\n",
       " 'exclude_hidden': True,\n",
       " 'required_exts': None,\n",
       " 'num_files_limit': None,\n",
       " 'raise_on_error': False,\n",
       " 'input_dir': WindowsPath('papers'),\n",
       " 'input_files': [WindowsPath('C:/Users/18623/Desktop/PhiAi/Jupyter/papers/attention is all you need 1706.03762v7.pdf'),\n",
       "  WindowsPath('C:/Users/18623/Desktop/PhiAi/Jupyter/papers/BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf')],\n",
       " 'file_extractor': {},\n",
       " 'file_metadata': <llama_index.core.readers.file.base._DefaultFileMetadataFunc at 0x16d49700f10>,\n",
       " 'filename_as_id': False}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader = SimpleDirectoryReader(input_dir=\"papers\", recursive=True)\n",
    "reader.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cd910b-9b01-4808-814a-189a964c18d5",
   "metadata": {},
   "source": [
    "<b>1.3 Store Text chunks in a dict to trace chunk-paper mapping</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f103d12c-8486-428d-bfb6-362c3d1ab93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract from file attention is all you need 1706.03762v7.pdf\n",
      "Extract from file BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf\n"
     ]
    }
   ],
   "source": [
    "pdf_files = reader.__dict__['input_files']\n",
    "all_docs = []\n",
    "doc_dict = {}\n",
    "for i,docs in enumerate(reader.iter_data()):\n",
    "    pdf_name = os.path.basename(pdf_files[i])\n",
    "    print(f'Extract from file {pdf_name}')\n",
    "    if pdf_name in doc_dict:\n",
    "        doc_dict[pdf_name] += [docs]\n",
    "    else:\n",
    "        doc_dict[pdf_name] = [docs]\n",
    "    all_docs.extend(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c40a926-8a10-4689-9562-1421eb3bf2fd",
   "metadata": {},
   "source": [
    "<b>1.4 Inspect Llama-Index document class</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e193598e-04e6-4312-94ed-f7ea50956e12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['attention is all you need 1706.03762v7.pdf', 'BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326b8a18-91a4-4a32-9739-93ba1a963d23",
   "metadata": {},
   "source": [
    "<b>1.5 Get total number of collected chunks</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "366114ae-55ab-4232-b16b-dcf6ea1c7054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25ff838-79a3-49cb-8ad4-5aacbfcf1eca",
   "metadata": {},
   "source": [
    "<b>1.6 Preprocess chunk to drop first page and  Reshape a way to store the Llama-index docs</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61432322-c86a-4b70-868e-73feeb5720cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = list(doc_dict.keys())\n",
    "\n",
    "for key in articles:\n",
    "    clean_docs = []\n",
    "    for doc in doc_dict[key][0]:\n",
    "        if 'See discussions, stats' in doc.text:\n",
    "            continue\n",
    "        clean_docs.append(doc)\n",
    "    doc_dict[key] = clean_docs\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca8b6ea-ae32-498f-8e75-1a683d2135c3",
   "metadata": {},
   "source": [
    "<b>1.7 Get chuck distribution accross papers</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dde5e148-5fce-4f61-8dc4-cf1296e93b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention is all you need 1706.03762v7.pdf >>> 15 chunks\n",
      "BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf >>> 16 chunks\n"
     ]
    }
   ],
   "source": [
    "for key in doc_dict:\n",
    "    num_chunks = len(doc_dict[key])\n",
    "    print(f'{key} >>> {num_chunks} chunks')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a672c9-3502-43c9-b213-30bb1cad9c6e",
   "metadata": {},
   "source": [
    "<b>1.8 Collect all chunks for each paper in one text</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "297e44ba-86fe-4058-ba90-e2c1bd33d35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = list(doc_dict.keys())\n",
    "papers = {} \n",
    "\n",
    "for paper_name in articles:\n",
    "    papers[paper_name] = ''\n",
    "    for doc in doc_dict[paper_name]:\n",
    "        papers[paper_name] += ' ' + doc.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "433a58a7-8658-414d-8c53-55fa7b0fcf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "for paper_name in articles:\n",
    "    papers[paper_name] = papers[paper_name].replace('.\\n','. ')\n",
    "    papers[paper_name] = papers[paper_name].replace('\\n',' ')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "844dd6dc-b501-4147-8a70-e66afe628c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#papers[articles[3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ea3e6d-38f5-49ee-8544-3d86461ce15d",
   "metadata": {},
   "source": [
    "## 2. QA: Ask questions about the papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6060b5-9d97-4b74-a2cb-9197e7232a33",
   "metadata": {},
   "source": [
    "<b>2.1 Roberta-base-sqaud2-distilled model</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7c1ea97-4f7f-4a06-a074-e964be5f9afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d0e93c3-c3d5-4eec-9773-90fdd155d303",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1489042f-95f5-4c91-8661-771912e01101",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"deepset/roberta-base-squad2-distilled\"\n",
    "\n",
    "# a) Get predictions\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "nlp = pipeline('question-answering', \n",
    "               model=model_name, \n",
    "               tokenizer=model_name,\n",
    "               device = device\n",
    "              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e24d267-2de0-45fb-b35f-55529428da0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\18623\\anaconda3\\envs\\transformers\\lib\\site-packages\\transformers\\pipelines\\question_answering.py:391: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\18623\\anaconda3\\envs\\transformers\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:370: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>model name</th>\n",
       "      <th>paper</th>\n",
       "      <th>answer</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what is attention?</td>\n",
       "      <td>deepset/roberta-base-squad2-distilled</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>mapping a query and a set of key-value pairs t...</td>\n",
       "      <td>0.356291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what is attention?</td>\n",
       "      <td>deepset/roberta-base-squad2-distilled</td>\n",
       "      <td>BERT pre_training of deep bidirectional transf...</td>\n",
       "      <td>all you need</td>\n",
       "      <td>0.802495</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             question                             model name  \\\n",
       "0  what is attention?  deepset/roberta-base-squad2-distilled   \n",
       "1  what is attention?  deepset/roberta-base-squad2-distilled   \n",
       "\n",
       "                                               paper  \\\n",
       "0         attention is all you need 1706.03762v7.pdf   \n",
       "1  BERT pre_training of deep bidirectional transf...   \n",
       "\n",
       "                                              answer     score  \n",
       "0  mapping a query and a set of key-value pairs t...  0.356291  \n",
       "1                                       all you need  0.802495  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_list = []\n",
    "score_list = []\n",
    "for paper in articles:\n",
    "    question = 'what is attention?'\n",
    "    QA_input = {\n",
    "        'question': question,\n",
    "        'context': papers[paper]\n",
    "    }\n",
    "    res = nlp(QA_input)\n",
    "    #print(f'from paper {paper} \\n we learn that answer to {question} is \\n {res} \\n')\n",
    "    answer_list.append(res['answer'])\n",
    "    score_list.append(res['score'])\n",
    "\n",
    "dfs[question + model_name] = pd.DataFrame({'question':len(articles)*[question],\n",
    "                                           'model name':len(articles)*[model_name],\n",
    "                                           'paper':articles,\n",
    "                                           'answer': answer_list,\n",
    "                                           'score':score_list\n",
    "                             })\n",
    "dfs[question + model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "519796ab-d401-4869-95c0-7b9307cff398",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\18623\\anaconda3\\envs\\transformers\\lib\\site-packages\\transformers\\pipelines\\question_answering.py:391: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>model name</th>\n",
       "      <th>paper</th>\n",
       "      <th>answer</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what types of attention are defined?</td>\n",
       "      <td>deepset/roberta-base-squad2-distilled</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>Structured attention networks</td>\n",
       "      <td>0.679690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what types of attention are defined?</td>\n",
       "      <td>deepset/roberta-base-squad2-distilled</td>\n",
       "      <td>BERT pre_training of deep bidirectional transf...</td>\n",
       "      <td>bidirectional self-attention</td>\n",
       "      <td>0.005291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               question  \\\n",
       "0  what types of attention are defined?   \n",
       "1  what types of attention are defined?   \n",
       "\n",
       "                              model name  \\\n",
       "0  deepset/roberta-base-squad2-distilled   \n",
       "1  deepset/roberta-base-squad2-distilled   \n",
       "\n",
       "                                               paper  \\\n",
       "0         attention is all you need 1706.03762v7.pdf   \n",
       "1  BERT pre_training of deep bidirectional transf...   \n",
       "\n",
       "                          answer     score  \n",
       "0  Structured attention networks  0.679690  \n",
       "1   bidirectional self-attention  0.005291  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_list = []\n",
    "score_list = []\n",
    "for paper in articles:\n",
    "    question = 'what types of attention are defined?'\n",
    "    QA_input = {\n",
    "        'question': question,\n",
    "        'context': papers[paper]\n",
    "    }\n",
    "    res = nlp(QA_input)\n",
    "    #print(f'from paper {paper} \\n we learn that answer to {question} is \\n {res} \\n')\n",
    "    answer_list.append(res['answer'])\n",
    "    score_list.append(res['score'])\n",
    "\n",
    "dfs[question + model_name] = pd.DataFrame({'question':len(articles)*[question],\n",
    "                              'model name':len(articles)*[model_name],\n",
    "                              'paper':articles,\n",
    "                              'answer': answer_list,\n",
    "                              'score':score_list\n",
    "                             })\n",
    "dfs[question + model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55a5c48f-782a-4ced-8d01-2bac57bff342",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\18623\\anaconda3\\envs\\transformers\\lib\\site-packages\\transformers\\pipelines\\question_answering.py:391: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>model name</th>\n",
       "      <th>paper</th>\n",
       "      <th>answer</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what is multi-head attention?</td>\n",
       "      <td>deepset/roberta-base-squad2-distilled</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>self-attention</td>\n",
       "      <td>0.623303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what is multi-head attention?</td>\n",
       "      <td>deepset/roberta-base-squad2-distilled</td>\n",
       "      <td>BERT pre_training of deep bidirectional transf...</td>\n",
       "      <td>multi-layered context</td>\n",
       "      <td>0.000016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        question                             model name  \\\n",
       "0  what is multi-head attention?  deepset/roberta-base-squad2-distilled   \n",
       "1  what is multi-head attention?  deepset/roberta-base-squad2-distilled   \n",
       "\n",
       "                                               paper                 answer  \\\n",
       "0         attention is all you need 1706.03762v7.pdf         self-attention   \n",
       "1  BERT pre_training of deep bidirectional transf...  multi-layered context   \n",
       "\n",
       "      score  \n",
       "0  0.623303  \n",
       "1  0.000016  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_list = []\n",
    "score_list = []\n",
    "for paper in articles:\n",
    "    question = 'what is multi-head attention?'\n",
    "    QA_input = {\n",
    "        'question': question,\n",
    "        'context': papers[paper]\n",
    "    }\n",
    "    res = nlp(QA_input)\n",
    "    #print(f'from paper {paper} \\n we learn that answer to {question} is \\n {res} \\n')\n",
    "    answer_list.append(res['answer'])\n",
    "    score_list.append(res['score'])\n",
    "\n",
    "dfs[question + model_name] = pd.DataFrame({'question':len(articles)*[question],\n",
    "                              'model name':len(articles)*[model_name],\n",
    "                              'paper':articles,\n",
    "                              'answer': answer_list,\n",
    "                              'score':score_list\n",
    "                             })\n",
    "dfs[question + model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f24ad9-b99b-47ed-97e6-a2d4c2efdd66",
   "metadata": {},
   "source": [
    "<b>2.2 HuggingFace examplar pipeline</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a6eb06fc-59bd-4e7c-91b4-c16fec3c215d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "qa_model = pipeline(\"question-answering\", \n",
    "                    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d994b336-ee03-48b7-90ce-7f62e667a4b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>model name</th>\n",
       "      <th>paper</th>\n",
       "      <th>answer</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what is attention?</td>\n",
       "      <td>default</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>multi-head</td>\n",
       "      <td>0.716957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what is attention?</td>\n",
       "      <td>default</td>\n",
       "      <td>BERT pre_training of deep bidirectional transf...</td>\n",
       "      <td>few parameters need to be learned from scratch</td>\n",
       "      <td>0.765959</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             question model name  \\\n",
       "0  what is attention?    default   \n",
       "1  what is attention?    default   \n",
       "\n",
       "                                               paper  \\\n",
       "0         attention is all you need 1706.03762v7.pdf   \n",
       "1  BERT pre_training of deep bidirectional transf...   \n",
       "\n",
       "                                           answer     score  \n",
       "0                                      multi-head  0.716957  \n",
       "1  few parameters need to be learned from scratch  0.765959  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_list = []\n",
    "score_list = []\n",
    "for paper in articles:\n",
    "    question = 'what is attention?'\n",
    "    res = qa_model(question = question, context = papers[paper])\n",
    "    #print(f'from paper {paper} \\n we learn that answer to {question} is \\n {res} \\n')\n",
    "    answer_list.append(res['answer'])\n",
    "    score_list.append(res['score'])\n",
    "\n",
    "dfs[question+'_nomodel'] = pd.DataFrame({'question':len(articles)*[question],\n",
    "                                         'model name': len(articles)*['default'],\n",
    "                                         'paper':articles,\n",
    "                                         'answer': answer_list,\n",
    "                                         'score':score_list\n",
    "                                        })\n",
    "dfs[question+'_nomodel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "42d94006-22f5-4efb-b245-0c39ee9b9ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>model name</th>\n",
       "      <th>paper</th>\n",
       "      <th>answer</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what types of attention are defined?</td>\n",
       "      <td>default</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>active memory</td>\n",
       "      <td>0.715720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what types of attention are defined?</td>\n",
       "      <td>default</td>\n",
       "      <td>BERT pre_training of deep bidirectional transf...</td>\n",
       "      <td>deeper self-attention</td>\n",
       "      <td>0.486192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               question model name  \\\n",
       "0  what types of attention are defined?    default   \n",
       "1  what types of attention are defined?    default   \n",
       "\n",
       "                                               paper                 answer  \\\n",
       "0         attention is all you need 1706.03762v7.pdf          active memory   \n",
       "1  BERT pre_training of deep bidirectional transf...  deeper self-attention   \n",
       "\n",
       "      score  \n",
       "0  0.715720  \n",
       "1  0.486192  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_list = []\n",
    "score_list = []\n",
    "for paper in articles:\n",
    "    question = 'what types of attention are defined?'\n",
    "    res = qa_model(question = question, context = papers[paper])\n",
    "    #print(f'from paper {paper} \\n we learn that answer to {question} is \\n {res} \\n')\n",
    "    answer_list.append(res['answer'])\n",
    "    score_list.append(res['score'])\n",
    "\n",
    "dfs[question+'_nomodel'] = pd.DataFrame({'question':len(articles)*[question],\n",
    "                                         'model name': len(articles)*['default'],\n",
    "                                         'paper':articles,\n",
    "                                         'answer': answer_list,\n",
    "                                         'score':score_list\n",
    "                                        })\n",
    "dfs[question+'_nomodel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5d7cd3bf-907b-4e04-8708-c0278ca42eb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>model name</th>\n",
       "      <th>paper</th>\n",
       "      <th>answer</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what is multi-head attention?</td>\n",
       "      <td>default</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>scaled dot-product attention</td>\n",
       "      <td>0.80240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what is multi-head attention?</td>\n",
       "      <td>default</td>\n",
       "      <td>BERT pre_training of deep bidirectional transf...</td>\n",
       "      <td>natural language understanding</td>\n",
       "      <td>0.47042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        question model name  \\\n",
       "0  what is multi-head attention?    default   \n",
       "1  what is multi-head attention?    default   \n",
       "\n",
       "                                               paper  \\\n",
       "0         attention is all you need 1706.03762v7.pdf   \n",
       "1  BERT pre_training of deep bidirectional transf...   \n",
       "\n",
       "                           answer    score  \n",
       "0    scaled dot-product attention  0.80240  \n",
       "1  natural language understanding  0.47042  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_list = []\n",
    "score_list = []\n",
    "for paper in articles:\n",
    "    question = 'what is multi-head attention?'\n",
    "    res = qa_model(question = question, context = papers[paper])\n",
    "    #print(f'from paper {paper} \\n we learn that answer to {question} is \\n {res} \\n')\n",
    "    answer_list.append(res['answer'])\n",
    "    score_list.append(res['score'])\n",
    "\n",
    "dfs[question+'_nomodel'] = pd.DataFrame({'question':len(articles)*[question],\n",
    "                                         'model name': len(articles)*['default'],\n",
    "                                         'paper':articles,\n",
    "                                         'answer': answer_list,\n",
    "                                         'score':score_list\n",
    "                                        })\n",
    "dfs[question+'_nomodel']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda20157-4d5d-4520-bfd9-0376b449031e",
   "metadata": {},
   "source": [
    "<b>2.3 Summarize QA results in one DataFrame</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "af28b227-e490-4a2a-b900-094aca62ba66",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_keys = list(dfs.keys())\n",
    "\n",
    "merged_df = pd.concat([dfs[key] for key in qa_keys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "132b54f6-8491-4401-8562-2bca226c2cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>model name</th>\n",
       "      <th>paper</th>\n",
       "      <th>answer</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what is attention?</td>\n",
       "      <td>deepset/roberta-base-squad2-distilled</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>mapping a query and a set of key-value pairs t...</td>\n",
       "      <td>0.356291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what is attention?</td>\n",
       "      <td>deepset/roberta-base-squad2-distilled</td>\n",
       "      <td>BERT pre_training of deep bidirectional transf...</td>\n",
       "      <td>all you need</td>\n",
       "      <td>0.802495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what types of attention are defined?</td>\n",
       "      <td>deepset/roberta-base-squad2-distilled</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>Structured attention networks</td>\n",
       "      <td>0.679690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what types of attention are defined?</td>\n",
       "      <td>deepset/roberta-base-squad2-distilled</td>\n",
       "      <td>BERT pre_training of deep bidirectional transf...</td>\n",
       "      <td>bidirectional self-attention</td>\n",
       "      <td>0.005291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what is multi-head attention?</td>\n",
       "      <td>deepset/roberta-base-squad2-distilled</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>self-attention</td>\n",
       "      <td>0.623303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what is multi-head attention?</td>\n",
       "      <td>deepset/roberta-base-squad2-distilled</td>\n",
       "      <td>BERT pre_training of deep bidirectional transf...</td>\n",
       "      <td>multi-layered context</td>\n",
       "      <td>0.000016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what is attention?</td>\n",
       "      <td>default</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>multi-head</td>\n",
       "      <td>0.716957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what is attention?</td>\n",
       "      <td>default</td>\n",
       "      <td>BERT pre_training of deep bidirectional transf...</td>\n",
       "      <td>few parameters need to be learned from scratch</td>\n",
       "      <td>0.765959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what types of attention are defined?</td>\n",
       "      <td>default</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>active memory</td>\n",
       "      <td>0.715720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what types of attention are defined?</td>\n",
       "      <td>default</td>\n",
       "      <td>BERT pre_training of deep bidirectional transf...</td>\n",
       "      <td>deeper self-attention</td>\n",
       "      <td>0.486192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what is multi-head attention?</td>\n",
       "      <td>default</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>scaled dot-product attention</td>\n",
       "      <td>0.802400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what is multi-head attention?</td>\n",
       "      <td>default</td>\n",
       "      <td>BERT pre_training of deep bidirectional transf...</td>\n",
       "      <td>natural language understanding</td>\n",
       "      <td>0.470420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               question  \\\n",
       "0                    what is attention?   \n",
       "1                    what is attention?   \n",
       "0  what types of attention are defined?   \n",
       "1  what types of attention are defined?   \n",
       "0         what is multi-head attention?   \n",
       "1         what is multi-head attention?   \n",
       "0                    what is attention?   \n",
       "1                    what is attention?   \n",
       "0  what types of attention are defined?   \n",
       "1  what types of attention are defined?   \n",
       "0         what is multi-head attention?   \n",
       "1         what is multi-head attention?   \n",
       "\n",
       "                              model name  \\\n",
       "0  deepset/roberta-base-squad2-distilled   \n",
       "1  deepset/roberta-base-squad2-distilled   \n",
       "0  deepset/roberta-base-squad2-distilled   \n",
       "1  deepset/roberta-base-squad2-distilled   \n",
       "0  deepset/roberta-base-squad2-distilled   \n",
       "1  deepset/roberta-base-squad2-distilled   \n",
       "0                                default   \n",
       "1                                default   \n",
       "0                                default   \n",
       "1                                default   \n",
       "0                                default   \n",
       "1                                default   \n",
       "\n",
       "                                               paper  \\\n",
       "0         attention is all you need 1706.03762v7.pdf   \n",
       "1  BERT pre_training of deep bidirectional transf...   \n",
       "0         attention is all you need 1706.03762v7.pdf   \n",
       "1  BERT pre_training of deep bidirectional transf...   \n",
       "0         attention is all you need 1706.03762v7.pdf   \n",
       "1  BERT pre_training of deep bidirectional transf...   \n",
       "0         attention is all you need 1706.03762v7.pdf   \n",
       "1  BERT pre_training of deep bidirectional transf...   \n",
       "0         attention is all you need 1706.03762v7.pdf   \n",
       "1  BERT pre_training of deep bidirectional transf...   \n",
       "0         attention is all you need 1706.03762v7.pdf   \n",
       "1  BERT pre_training of deep bidirectional transf...   \n",
       "\n",
       "                                              answer     score  \n",
       "0  mapping a query and a set of key-value pairs t...  0.356291  \n",
       "1                                       all you need  0.802495  \n",
       "0                      Structured attention networks  0.679690  \n",
       "1                       bidirectional self-attention  0.005291  \n",
       "0                                     self-attention  0.623303  \n",
       "1                              multi-layered context  0.000016  \n",
       "0                                         multi-head  0.716957  \n",
       "1     few parameters need to be learned from scratch  0.765959  \n",
       "0                                      active memory  0.715720  \n",
       "1                              deeper self-attention  0.486192  \n",
       "0                       scaled dot-product attention  0.802400  \n",
       "1                     natural language understanding  0.470420  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "35f41f47-f34b-477b-aeff-b9e6f63bd78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['index', 'question', 'model name', 'paper', 'answer', 'score']\n"
     ]
    }
   ],
   "source": [
    "merged_df = merged_df.reset_index()\n",
    "cols = list(merged_df.columns)\n",
    "print(cols)\n",
    "cols.remove('index')\n",
    "merged_df = merged_df.groupby(['question','model name'])[cols].apply(lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b57d17ab-01eb-454e-b8a4-a197521297b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>model name</th>\n",
       "      <th>paper</th>\n",
       "      <th>answer</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question</th>\n",
       "      <th>model name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">what is attention?</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">deepset/roberta-base-squad2-distilled</th>\n",
       "      <th>0</th>\n",
       "      <td>what is attention?</td>\n",
       "      <td>deepset/roberta-base-squad2-distilled</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>mapping a query and a set of key-value pairs t...</td>\n",
       "      <td>0.356291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what is attention?</td>\n",
       "      <td>deepset/roberta-base-squad2-distilled</td>\n",
       "      <td>BERT pre_training of deep bidirectional transf...</td>\n",
       "      <td>all you need</td>\n",
       "      <td>0.802495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">default</th>\n",
       "      <th>6</th>\n",
       "      <td>what is attention?</td>\n",
       "      <td>default</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>multi-head</td>\n",
       "      <td>0.716957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>what is attention?</td>\n",
       "      <td>default</td>\n",
       "      <td>BERT pre_training of deep bidirectional transf...</td>\n",
       "      <td>few parameters need to be learned from scratch</td>\n",
       "      <td>0.765959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">what is multi-head attention?</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">deepset/roberta-base-squad2-distilled</th>\n",
       "      <th>4</th>\n",
       "      <td>what is multi-head attention?</td>\n",
       "      <td>deepset/roberta-base-squad2-distilled</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>self-attention</td>\n",
       "      <td>0.623303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>what is multi-head attention?</td>\n",
       "      <td>deepset/roberta-base-squad2-distilled</td>\n",
       "      <td>BERT pre_training of deep bidirectional transf...</td>\n",
       "      <td>multi-layered context</td>\n",
       "      <td>0.000016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">default</th>\n",
       "      <th>10</th>\n",
       "      <td>what is multi-head attention?</td>\n",
       "      <td>default</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>scaled dot-product attention</td>\n",
       "      <td>0.802400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>what is multi-head attention?</td>\n",
       "      <td>default</td>\n",
       "      <td>BERT pre_training of deep bidirectional transf...</td>\n",
       "      <td>natural language understanding</td>\n",
       "      <td>0.470420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">what types of attention are defined?</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">deepset/roberta-base-squad2-distilled</th>\n",
       "      <th>2</th>\n",
       "      <td>what types of attention are defined?</td>\n",
       "      <td>deepset/roberta-base-squad2-distilled</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>Structured attention networks</td>\n",
       "      <td>0.679690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what types of attention are defined?</td>\n",
       "      <td>deepset/roberta-base-squad2-distilled</td>\n",
       "      <td>BERT pre_training of deep bidirectional transf...</td>\n",
       "      <td>bidirectional self-attention</td>\n",
       "      <td>0.005291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">default</th>\n",
       "      <th>8</th>\n",
       "      <td>what types of attention are defined?</td>\n",
       "      <td>default</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>active memory</td>\n",
       "      <td>0.715720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>what types of attention are defined?</td>\n",
       "      <td>default</td>\n",
       "      <td>BERT pre_training of deep bidirectional transf...</td>\n",
       "      <td>deeper self-attention</td>\n",
       "      <td>0.486192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                           question  \\\n",
       "question                             model name                                                                       \n",
       "what is attention?                   deepset/roberta-base-squad2-distilled 0                     what is attention?   \n",
       "                                                                           1                     what is attention?   \n",
       "                                     default                               6                     what is attention?   \n",
       "                                                                           7                     what is attention?   \n",
       "what is multi-head attention?        deepset/roberta-base-squad2-distilled 4          what is multi-head attention?   \n",
       "                                                                           5          what is multi-head attention?   \n",
       "                                     default                               10         what is multi-head attention?   \n",
       "                                                                           11         what is multi-head attention?   \n",
       "what types of attention are defined? deepset/roberta-base-squad2-distilled 2   what types of attention are defined?   \n",
       "                                                                           3   what types of attention are defined?   \n",
       "                                     default                               8   what types of attention are defined?   \n",
       "                                                                           9   what types of attention are defined?   \n",
       "\n",
       "                                                                                                          model name  \\\n",
       "question                             model name                                                                        \n",
       "what is attention?                   deepset/roberta-base-squad2-distilled 0   deepset/roberta-base-squad2-distilled   \n",
       "                                                                           1   deepset/roberta-base-squad2-distilled   \n",
       "                                     default                               6                                 default   \n",
       "                                                                           7                                 default   \n",
       "what is multi-head attention?        deepset/roberta-base-squad2-distilled 4   deepset/roberta-base-squad2-distilled   \n",
       "                                                                           5   deepset/roberta-base-squad2-distilled   \n",
       "                                     default                               10                                default   \n",
       "                                                                           11                                default   \n",
       "what types of attention are defined? deepset/roberta-base-squad2-distilled 2   deepset/roberta-base-squad2-distilled   \n",
       "                                                                           3   deepset/roberta-base-squad2-distilled   \n",
       "                                     default                               8                                 default   \n",
       "                                                                           9                                 default   \n",
       "\n",
       "                                                                                                                           paper  \\\n",
       "question                             model name                                                                                    \n",
       "what is attention?                   deepset/roberta-base-squad2-distilled 0          attention is all you need 1706.03762v7.pdf   \n",
       "                                                                           1   BERT pre_training of deep bidirectional transf...   \n",
       "                                     default                               6          attention is all you need 1706.03762v7.pdf   \n",
       "                                                                           7   BERT pre_training of deep bidirectional transf...   \n",
       "what is multi-head attention?        deepset/roberta-base-squad2-distilled 4          attention is all you need 1706.03762v7.pdf   \n",
       "                                                                           5   BERT pre_training of deep bidirectional transf...   \n",
       "                                     default                               10         attention is all you need 1706.03762v7.pdf   \n",
       "                                                                           11  BERT pre_training of deep bidirectional transf...   \n",
       "what types of attention are defined? deepset/roberta-base-squad2-distilled 2          attention is all you need 1706.03762v7.pdf   \n",
       "                                                                           3   BERT pre_training of deep bidirectional transf...   \n",
       "                                     default                               8          attention is all you need 1706.03762v7.pdf   \n",
       "                                                                           9   BERT pre_training of deep bidirectional transf...   \n",
       "\n",
       "                                                                                                                          answer  \\\n",
       "question                             model name                                                                                    \n",
       "what is attention?                   deepset/roberta-base-squad2-distilled 0   mapping a query and a set of key-value pairs t...   \n",
       "                                                                           1                                        all you need   \n",
       "                                     default                               6                                          multi-head   \n",
       "                                                                           7      few parameters need to be learned from scratch   \n",
       "what is multi-head attention?        deepset/roberta-base-squad2-distilled 4                                      self-attention   \n",
       "                                                                           5                               multi-layered context   \n",
       "                                     default                               10                       scaled dot-product attention   \n",
       "                                                                           11                     natural language understanding   \n",
       "what types of attention are defined? deepset/roberta-base-squad2-distilled 2                       Structured attention networks   \n",
       "                                                                           3                        bidirectional self-attention   \n",
       "                                     default                               8                                       active memory   \n",
       "                                                                           9                               deeper self-attention   \n",
       "\n",
       "                                                                                  score  \n",
       "question                             model name                                          \n",
       "what is attention?                   deepset/roberta-base-squad2-distilled 0   0.356291  \n",
       "                                                                           1   0.802495  \n",
       "                                     default                               6   0.716957  \n",
       "                                                                           7   0.765959  \n",
       "what is multi-head attention?        deepset/roberta-base-squad2-distilled 4   0.623303  \n",
       "                                                                           5   0.000016  \n",
       "                                     default                               10  0.802400  \n",
       "                                                                           11  0.470420  \n",
       "what types of attention are defined? deepset/roberta-base-squad2-distilled 2   0.679690  \n",
       "                                                                           3   0.005291  \n",
       "                                     default                               8   0.715720  \n",
       "                                                                           9   0.486192  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64f8ce2-30dc-4e97-a724-b2325e3655a9",
   "metadata": {},
   "source": [
    "## 3. Sematic Vector Search: Hands-On "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1f486ce6-6467-4cd6-a5ac-01d4ca7226a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e4632c2d-657a-4aff-8f57-4291833fc99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'Snowflake/snowflake-arctic-embed-l-v2.0'\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5797c17e-fa1e-4a51-81ff-529bdbbc6107",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = ['what is attention?', 'what is multi-head attention?']\n",
    "documents = [] \n",
    "for rec in all_docs:\n",
    "    text_chunk = rec.text.replace('.\\n','. ')\n",
    "    text_chunk = text_chunk.replace('\\n',' ')\n",
    "    documents.append(text_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bde66917-acef-4938-ad6e-3b0b52a25c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embeddings = model.encode(queries, prompt_name=\"query\") \n",
    "document_embeddings = model.encode(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "372c57d4-dd9e-4ed4-8506-92fafa2072f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.similarity(query_embeddings, document_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1581f92f-de10-4d3c-98ef-5b2d140cb725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.3195346 ,  0.24990332,  0.28661358,  0.3212555 ,  0.26088548,\n",
       "         0.13312666,  0.13690537, -0.01953743,  0.07883323,  0.21618257,\n",
       "         0.15051648,  0.13195562,  0.31160092,  0.28166437,  0.24416181,\n",
       "         0.0802324 ,  0.05307864,  0.11930332,  0.04431819,  0.11695188,\n",
       "         0.07378677,  0.0697437 ,  0.02865066,  0.08885486,  0.0631457 ,\n",
       "         0.12509821,  0.09325863,  0.03556471,  0.08303171,  0.09524073,\n",
       "         0.02277639],\n",
       "       [ 0.37941754,  0.411205  ,  0.39959958,  0.4731409 ,  0.55398375,\n",
       "         0.25181007,  0.31452248,  0.15422355,  0.28774974,  0.36324242,\n",
       "         0.26917607,  0.25555977,  0.35022897,  0.3023172 ,  0.30858317,\n",
       "         0.23788661,  0.21420331,  0.2624512 ,  0.20118485,  0.24324313,\n",
       "         0.17298877,  0.18839133,  0.2460844 ,  0.29325908,  0.21080701,\n",
       "         0.25936761,  0.29565617,  0.19223814,  0.19146955,  0.2046683 ,\n",
       "         0.21918511]], dtype=float32)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d53a28ba-f5af-48f7-a6ce-12a3d56dc733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: what is attention?\n",
      "0.3212555 Scaled Dot-Product Attention  Multi-Head Attention Figure 2: (left) Scaled Dot-Product Attention. (r\n",
      "0.3195346 Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and\n",
      "0.31160092 Attention Visualizations Input-Input Layer5 It is in this spirit that a majority of American governm\n",
      "0.28661358 Figure 1: The Transformer - model architecture. The Transformer follows this overall architecture us\n",
      "0.28166437 Input-Input Layer5 The Law will never be perfect , but its application should be just - this is what\n",
      "0.26088548 output values. These are concatenated and once again projected, resulting in the final values, as de\n",
      "Query: what is multi-head attention?\n",
      "0.55398375 output values. These are concatenated and once again projected, resulting in the final values, as de\n",
      "0.4731409 Scaled Dot-Product Attention  Multi-Head Attention Figure 2: (left) Scaled Dot-Product Attention. (r\n",
      "0.411205 1 Introduction Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural\n",
      "0.39959958 Figure 1: The Transformer - model architecture. The Transformer follows this overall architecture us\n",
      "0.37941754 Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and\n",
      "0.36324242 Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\n",
      "0.35022897 Attention Visualizations Input-Input Layer5 It is in this spirit that a majority of American governm\n",
      "0.31452248 length n is smaller than the representation dimensionality d, which is most often the case with sent\n",
      "0.30858317 Input-Input Layer5 The Law will never be perfect , but its application should be just - this is what\n",
      "0.3023172 Input-Input Layer5 The Law will never be perfect , but its application should be just - this is what\n",
      "0.29565617 for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: An- alyzi\n",
      "0.29325908 mixed results on the downstream task impact of increasing the pre-trained bi-LM size from two to fou\n",
      "0.28774974 Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the b\n",
      "0.26917607 [5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua\n",
      "0.2624512 BERT BERT E[CLS] E1  E[SEP]... EN E1 ... EM C  T1  T[SEP]...  TN  T1 ...  TM [CLS] Tok 1  [SEP].\n",
      "0.25936761 Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distan\n",
      "0.25555977 [25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated c\n",
      "0.25181007 Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for \n"
     ]
    }
   ],
   "source": [
    "# Output the results\n",
    "thres = 0.25\n",
    "\n",
    "query_score_list = []\n",
    "\n",
    "for query, query_scores in zip(queries, scores):\n",
    "    doc_score_pairs = list(zip(documents, query_scores))\n",
    "    doc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n",
    "    print(\"Query:\", query)\n",
    "    k=-1\n",
    "    for document, score in doc_score_pairs:\n",
    "        k += 1\n",
    "        score_val = score.numpy()\n",
    "        if score_val >= 0.25:\n",
    "            print(score_val, document[:100])\n",
    "            query_score_list.append({'query':query, \n",
    "                                     'doc':document,\n",
    "                                     'llama_index_doc': all_docs[k],\n",
    "                                     'score': score_val\n",
    "                                    })        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "90646135-9446-4fcb-9069-de97c3dfc240",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 7\n",
    "doc = query_score_list[ind]['doc']\n",
    "q = query_score_list[ind]['query']\n",
    "\n",
    "doc_sentence = doc.split('. ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cc193017-d1d3-42f4-a843-fd2cdb3266ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what is multi-head attention?'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3ed1a270-7bcc-458c-9979-b67e3f0da71b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Scaled Dot-Product Attention  Multi-Head Attention Figure 2: (left) Scaled Dot-Product Attention',\n",
       " '(right) Multi-Head Attention consists of several attention layers running in parallel',\n",
       " 'of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key',\n",
       " '3.2.1 Scaled Dot-Product Attention We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2)',\n",
       " 'The input consists of queries and keys of dimension dk, and values of dimension dv',\n",
       " 'We compute the dot products of the query with all keys, divide each by dk, and apply a softmax function to obtain the weights on the values',\n",
       " 'In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q',\n",
       " 'The keys and values are also packed together into matrices K and V ',\n",
       " 'We compute the matrix of outputs as: Attention(Q, K, V) = softmax(QKT dk )V (1) The two most commonly used attention functions are additive attention [2], and dot-product (multi- plicative) attention',\n",
       " 'Dot-product attention is identical to our algorithm, except for the scaling factor of 1dk ',\n",
       " 'Additive attention computes the compatibility function using a feed-forward network with a single hidden layer',\n",
       " 'While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code',\n",
       " 'While for small values of dk the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk [3]',\n",
       " 'We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients 4',\n",
       " 'To counteract this effect, we scale the dot products by 1dk ',\n",
       " '3.2.2 Multi-Head Attention Instead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively',\n",
       " 'On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional 4To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1',\n",
       " 'Then their dot product, q  k = Pdk i=1 qiki, has mean 0 and variance dk',\n",
       " '4']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6d76fc92-58cc-4306-8a84-7803f11cd11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_query_embeddings = model.encode(q, prompt_name=\"query\") \n",
    "doc_sentence_embeddings = model.encode(doc_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "94c29d14-16a0-41dc-b757-42f18d7aacf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_scores = model.similarity(doc_query_embeddings, \n",
    "                                   doc_sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "448d1205-c7f3-403f-885f-d5477486c1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "import matplotlib as matplotlib\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "def color_map_color(value, \n",
    "                    cmap_name='PuBu',\n",
    "                    #cmap_name='Wistia', \n",
    "                    vmin=0, \n",
    "                    vmax=1):\n",
    "    # norm = plt.Normalize(vmin, vmax)\n",
    "    norm = matplotlib.colors.Normalize(vmin=vmin, vmax=vmax)\n",
    "    cmap = cm.get_cmap(cmap_name)  # PiYG\n",
    "    rgb = cmap(norm(abs(value)))[:3]  # will return rgba, we take only first 3 so we get rgb\n",
    "    color = matplotlib.colors.rgb2hex(rgb)\n",
    "    return color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e5e8632b-06f9-4fbb-91fe-2d3b09e2bd15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\18623\\AppData\\Local\\Temp\\ipykernel_1472\\4137129256.py:12: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  cmap = cm.get_cmap(cmap_name)  # PiYG\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'#6fa7ce'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "color_map_color(0.51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "170654e9-32be-46da-8e03-53bf6d15695a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\18623\\AppData\\Local\\Temp\\ipykernel_1472\\4137129256.py:12: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  cmap = cm.get_cmap(cmap_name)  # PiYG\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"<span style='background-color:#6fa7ce'>text</span>\""
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_template = \"<span style='background-color:{}'>{}</span>\"\n",
    "html_template.format(color_map_color(0.51),'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "614db170-8459-4459-b0aa-fcd5bf5f1fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\18623\\AppData\\Local\\Temp\\ipykernel_1472\\4137129256.py:12: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  cmap = cm.get_cmap(cmap_name)  # PiYG\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<span style='background-color:#6fa7ce'>text</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(html_template.format(color_map_color(0.51),'text')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f3035211-c33d-4735-b95b-7fde23b54f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\18623\\AppData\\Local\\Temp\\ipykernel_1472\\4137129256.py:12: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  cmap = cm.get_cmap(cmap_name)  # PiYG\n"
     ]
    }
   ],
   "source": [
    "html_template = \"<span style='background-color:{};opacity:0.8;'>{}</span><sup style='font-size:10px;font-weight:bold;color:red;'>Score:{:.2f}</sup>\"\n",
    "html_output = []\n",
    "for query, query_scores in zip(q, sentence_scores):\n",
    "    doc_score_pairs = list(zip(doc_sentence, query_scores))\n",
    "    doc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n",
    "    #print(\"Query:\", q)\n",
    "    for document, score in doc_score_pairs:\n",
    "        score_val = score.numpy()\n",
    "        html_output.append(html_template.format(color_map_color(1-score_val),document,score_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3c42aeac-4f85-4d88-8752-27f3401b6f77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span style='background-color:#c4cbe3;opacity:0.8;'>(right) Multi-Head Attention consists of several attention layers running in parallel</span><sup style='font-size:10px;font-weight:bold;color:red;'>Score:0.71</sup> <span style='background-color:#79abd0;opacity:0.8;'>3.2.2 Multi-Head Attention Instead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively</span><sup style='font-size:10px;font-weight:bold;color:red;'>Score:0.51</sup> <span style='background-color:#78abd0;opacity:0.8;'>Scaled Dot-Product Attention  Multi-Head Attention Figure 2: (left) Scaled Dot-Product Attention</span><sup style='font-size:10px;font-weight:bold;color:red;'>Score:0.51</sup> <span style='background-color:#509ac6;opacity:0.8;'>In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q</span><sup style='font-size:10px;font-weight:bold;color:red;'>Score:0.43</sup> <span style='background-color:#4295c3;opacity:0.8;'>We compute the matrix of outputs as: Attention(Q, K, V) = softmax(QKT dk )V (1) The two most commonly used attention functions are additive attention [2], and dot-product (multi- plicative) attention</span><sup style='font-size:10px;font-weight:bold;color:red;'>Score:0.40</sup> <span style='background-color:#3f93c2;opacity:0.8;'>Additive attention computes the compatibility function using a feed-forward network with a single hidden layer</span><sup style='font-size:10px;font-weight:bold;color:red;'>Score:0.39</sup> <span style='background-color:#2a88bc;opacity:0.8;'>While for small values of dk the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk [3]</span><sup style='font-size:10px;font-weight:bold;color:red;'>Score:0.35</sup> <span style='background-color:#2987bc;opacity:0.8;'>While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code</span><sup style='font-size:10px;font-weight:bold;color:red;'>Score:0.34</sup> <span style='background-color:#2383ba;opacity:0.8;'>On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional 4To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1</span><sup style='font-size:10px;font-weight:bold;color:red;'>Score:0.32</sup> <span style='background-color:#2081b9;opacity:0.8;'>3.2.1 Scaled Dot-Product Attention We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2)</span><sup style='font-size:10px;font-weight:bold;color:red;'>Score:0.32</sup> <span style='background-color:#2081b9;opacity:0.8;'>Dot-product attention is identical to our algorithm, except for the scaling factor of 1dk </span><sup style='font-size:10px;font-weight:bold;color:red;'>Score:0.32</sup> <span style='background-color:#045382;opacity:0.8;'>To counteract this effect, we scale the dot products by 1dk </span><sup style='font-size:10px;font-weight:bold;color:red;'>Score:0.10</sup> <span style='background-color:#045280;opacity:0.8;'>We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients 4</span><sup style='font-size:10px;font-weight:bold;color:red;'>Score:0.10</sup> <span style='background-color:#034c78;opacity:0.8;'>4</span><sup style='font-size:10px;font-weight:bold;color:red;'>Score:0.08</sup> <span style='background-color:#034c78;opacity:0.8;'>The input consists of queries and keys of dimension dk, and values of dimension dv</span><sup style='font-size:10px;font-weight:bold;color:red;'>Score:0.07</sup> <span style='background-color:#034a74;opacity:0.8;'>of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key</span><sup style='font-size:10px;font-weight:bold;color:red;'>Score:0.07</sup> <span style='background-color:#034973;opacity:0.8;'>We compute the dot products of the query with all keys, divide each by dk, and apply a softmax function to obtain the weights on the values</span><sup style='font-size:10px;font-weight:bold;color:red;'>Score:0.06</sup> <span style='background-color:#03456c;opacity:0.8;'>Then their dot product, q  k = Pdk i=1 qiki, has mean 0 and variance dk</span><sup style='font-size:10px;font-weight:bold;color:red;'>Score:0.05</sup> <span style='background-color:#034369;opacity:0.8;'>The keys and values are also packed together into matrices K and V </span><sup style='font-size:10px;font-weight:bold;color:red;'>Score:0.04</sup>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(' '.join(html_output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758c55fc-356d-4f6a-b183-e4d52233052b",
   "metadata": {},
   "source": [
    "## 4. Semantic Vector Search: Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "58bf1575-0e70-4e69-87cf-90f9aec2997e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "98ddb757-ef90-4b48-b292-6f8864b79471",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "624445b2-348a-4d59-ba8f-746465c16550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store docs into vector DB\n",
    "index = VectorStoreIndex.from_documents(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "35f22dec-7949-42ef-9b09-ca5c5274918f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of docs to retreive\n",
    "top_k = 7\n",
    "\n",
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=top_k,\n",
    ")\n",
    "     \n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.3)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76b6c14-fc5c-4268-86ed-20d486edcfc7",
   "metadata": {},
   "source": [
    "<b>4.1 To BERT or not to BERT? this is the questions</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "91e045ed-8627-47e5-886b-3d812f05c895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query documents\n",
    "query = \"What is BERT?\"\n",
    "response = query_engine.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "7c4c489a-81c7-4903-8b73-3a47f50cc171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['response', 'source_nodes', 'metadata'])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "25cd522b-6362-45d7-bafe-3520aab862a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.__dict__['source_nodes'][0].node.__dict__['metadata']['file_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "0d3e2543-eb6a-4de2-b112-a4e6b5dcb847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat response\n",
    "cols = ['question','scores','source','text']\n",
    "df_dict = {col:[] for col in cols}\n",
    "\n",
    "for i in range(top_k):\n",
    "    #context += f'>>> chunk {i+1} ' + str(response.source_nodes[i].score) + '\\n\\n' + response.source_nodes[i].text + \"\\n\\n\"\n",
    "    #context += 'From: '+response.__dict__['source_nodes'][i].node.__dict__['metadata']['file_name'] + '\\n\\n'\n",
    "    df_dict['scores'].append(response.source_nodes[i].score)\n",
    "    df_dict['source'].append(response.__dict__['source_nodes'][i].node.__dict__['metadata']['file_name'])\n",
    "    df_dict['text'].append(response.source_nodes[i].text)\n",
    "\n",
    "df_dict['question'] = [query]*len(df_dict['scores'])\n",
    "output_df[query] = pd.DataFrame(df_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "9a88cfaa-2206-4ddc-b661-03e12416a13a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>scores</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is BERT?</td>\n",
       "      <td>0.811123</td>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>BERT is the rst ne-\\ntuning based representation model that achieves\\nstate-of-the-art performance on a large suite\\nof sentence-level and token-level tasks, outper-\\nforming many task-specic architectures.\\n BERT advances the state of the art for eleven\\nNLP tasks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is BERT?</td>\n",
       "      <td>0.751915</td>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>BERT is conceptually simple and empirically\\npowerful. It obtains new state-of-the-art re-\\nsults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is BERT?</td>\n",
       "      <td>0.751781</td>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>BERT BERT\\nE[CLS] E1  E[SEP]. EN E1 . EM\\nC\\n T1\\n T[SEP].\\n TN\\n T1 .\\n TM\\n[CLS] Tok 1  [SEP]. Tok N Tok 1 . TokM\\nQuestion Paragraph\\nStart/End Span\\nBERT\\nE[CLS] E1  E[SEP].</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is BERT?</td>\n",
       "      <td>0.740903</td>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>TokM\\nQuestion Paragraph\\nStart/End Span\\nBERT\\nE[CLS] E1  E[SEP]. EN E1 . EM\\nC\\n T1\\n T[SEP].\\n TN\\n T1 .\\n TM\\n[CLS] Tok 1  [SEP]. Tok N Tok 1 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is BERT?</td>\n",
       "      <td>0.738021</td>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>Input/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g., Question, Answer ) in one token sequence.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What is BERT?</td>\n",
       "      <td>0.733836</td>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>Each downstream task has sep-\\narate ne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\nA distinctive feature of BERT is its unied ar-\\nchitecture across different tasks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What is BERT?</td>\n",
       "      <td>0.731738</td>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>BERT\\nE[CLS] E1  E[SEP]. EN E1 . EM\\nC\\n T1\\n T[SEP].\\n TN\\n T1 .\\n TM\\n[CLS] Tok \\n1\\n [SEP]. Tok \\nN\\nTok \\n1 .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        question    scores  \\\n",
       "0  What is BERT?  0.811123   \n",
       "1  What is BERT?  0.751915   \n",
       "2  What is BERT?  0.751781   \n",
       "3  What is BERT?  0.740903   \n",
       "4  What is BERT?  0.738021   \n",
       "5  What is BERT?  0.733836   \n",
       "6  What is BERT?  0.731738   \n",
       "\n",
       "                                                                                             source  \\\n",
       "0  BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "1  BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "2  BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "3  BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "4  BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "5  BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "6  BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                   text  \n",
       "0                                        BERT is the rst ne-\\ntuning based representation model that achieves\\nstate-of-the-art performance on a large suite\\nof sentence-level and token-level tasks, outper-\\nforming many task-specic architectures.\\n BERT advances the state of the art for eleven\\nNLP tasks.  \n",
       "1                     BERT is conceptually simple and empirically\\npowerful. It obtains new state-of-the-art re-\\nsults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.  \n",
       "2                                                                                                                                 BERT BERT\\nE[CLS] E1  E[SEP]. EN E1 . EM\\nC\\n T1\\n T[SEP].\\n TN\\n T1 .\\n TM\\n[CLS] Tok 1  [SEP]. Tok N Tok 1 . TokM\\nQuestion Paragraph\\nStart/End Span\\nBERT\\nE[CLS] E1  E[SEP].  \n",
       "3                                                                                                                                                               TokM\\nQuestion Paragraph\\nStart/End Span\\nBERT\\nE[CLS] E1  E[SEP]. EN E1 . EM\\nC\\n T1\\n T[SEP].\\n TN\\n T1 .\\n TM\\n[CLS] Tok 1  [SEP]. Tok N Tok 1 .  \n",
       "4                                                                    Input/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g., Question, Answer ) in one token sequence.  \n",
       "5  Each downstream task has sep-\\narate ne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\nA distinctive feature of BERT is its unied ar-\\nchitecture across different tasks.  \n",
       "6                                                                                                                                                                                                 BERT\\nE[CLS] E1  E[SEP]. EN E1 . EM\\nC\\n T1\\n T[SEP].\\n TN\\n T1 .\\n TM\\n[CLS] Tok \\n1\\n [SEP]. Tok \\nN\\nTok \\n1 .  "
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df[query]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1566ddbb-1628-45cc-ba2a-1b3befd843f5",
   "metadata": {},
   "source": [
    "<b>4.2 Pay Attention</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "329672a0-323a-4aa9-a804-e0bb8c7b8474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query documents\n",
    "query = \"What is multi-head attention?\"\n",
    "response = query_engine.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "cf687120-e8e8-4939-9e0c-ae076cfba69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat response\n",
    "cols = ['question','scores','source','text']\n",
    "df_dict = {col:[] for col in cols}\n",
    "\n",
    "for i in range(top_k):\n",
    "    #context += f'>>> chunk {i+1} ' + str(response.source_nodes[i].score) + '\\n\\n' + response.source_nodes[i].text + \"\\n\\n\"\n",
    "    #context += 'From: '+response.__dict__['source_nodes'][i].node.__dict__['metadata']['file_name'] + '\\n\\n'\n",
    "    df_dict['scores'].append(response.source_nodes[i].score)\n",
    "    df_dict['source'].append(response.__dict__['source_nodes'][i].node.__dict__['metadata']['file_name'])\n",
    "    df_dict['text'].append(response.source_nodes[i].text)\n",
    "\n",
    "df_dict['question'] = [query]*len(df_dict['scores'])\n",
    "output_df[query] = pd.DataFrame(df_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "531498e4-f0b4-43d5-afb5-a54bc1f1bc2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>scores</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is multi-head attention?</td>\n",
       "      <td>0.800710</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is multi-head attention?</td>\n",
       "      <td>0.784158</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is multi-head attention?</td>\n",
       "      <td>0.772050</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is multi-head attention?</td>\n",
       "      <td>0.758912</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\nwhere headi = Attention(QWQ\\ni , KWK\\ni , V WV\\ni )\\nWhere the projections are parameter matricesWQ\\ni  Rdmodeldk , WK\\ni  Rdmodeldk , WV\\ni  Rdmodeldv\\nand WO  Rhdvdmodel .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is multi-head attention?</td>\n",
       "      <td>0.740496</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [ 1].</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What is multi-head attention?</td>\n",
       "      <td>0.738943</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>To counteract this effect, we scale the dot products by 1dk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What is multi-head attention?</td>\n",
       "      <td>0.738749</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>In this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        question    scores  \\\n",
       "0  What is multi-head attention?  0.800710   \n",
       "1  What is multi-head attention?  0.784158   \n",
       "2  What is multi-head attention?  0.772050   \n",
       "3  What is multi-head attention?  0.758912   \n",
       "4  What is multi-head attention?  0.740496   \n",
       "5  What is multi-head attention?  0.738943   \n",
       "6  What is multi-head attention?  0.738749   \n",
       "\n",
       "                                       source  \\\n",
       "0  attention is all you need 1706.03762v7.pdf   \n",
       "1  attention is all you need 1706.03762v7.pdf   \n",
       "2  attention is all you need 1706.03762v7.pdf   \n",
       "3  attention is all you need 1706.03762v7.pdf   \n",
       "4  attention is all you need 1706.03762v7.pdf   \n",
       "5  attention is all you need 1706.03762v7.pdf   \n",
       "6  attention is all you need 1706.03762v7.pdf   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                               text  \n",
       "0                                             Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.  \n",
       "1  3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence.  \n",
       "2                                                          output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.  \n",
       "3                                                                                    With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\nwhere headi = Attention(QWQ\\ni , KWK\\ni , V WV\\ni )\\nWhere the projections are parameter matricesWQ\\ni  Rdmodeldk , WK\\ni  Rdmodeldk , WV\\ni  Rdmodeldv\\nand WO  Rhdvdmodel .  \n",
       "4                                                                                              Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [ 1].  \n",
       "5          To counteract this effect, we scale the dot products by 1dk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively.  \n",
       "6                                                                                                               In this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.  "
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df[query]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bce1b1-2a1a-4f39-89fe-cbf6d3156da0",
   "metadata": {},
   "source": [
    "<b>4.3 Who let Transformer out?</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "2263b187-6cda-417f-ad26-49a7fa71fa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query documents\n",
    "query = \"What is Transformer?\"\n",
    "response = query_engine.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "45782111-c557-4941-8a31-fd2a4590d5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat response\n",
    "cols = ['question','scores','source','text']\n",
    "df_dict = {col:[] for col in cols}\n",
    "\n",
    "for i in range(top_k):\n",
    "    #context += f'>>> chunk {i+1} ' + str(response.source_nodes[i].score) + '\\n\\n' + response.source_nodes[i].text + \"\\n\\n\"\n",
    "    #context += 'From: '+response.__dict__['source_nodes'][i].node.__dict__['metadata']['file_name'] + '\\n\\n'\n",
    "    df_dict['scores'].append(response.source_nodes[i].score)\n",
    "    df_dict['source'].append(response.__dict__['source_nodes'][i].node.__dict__['metadata']['file_name'])\n",
    "    df_dict['text'].append(response.source_nodes[i].text)\n",
    "\n",
    "df_dict['question'] = [query]*len(df_dict['scores'])\n",
    "output_df[query] = pd.DataFrame(df_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "7783af40-4677-4123-9f18-d7026e9fe697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>scores</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is Transformer?</td>\n",
       "      <td>0.696613</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is Transformer?</td>\n",
       "      <td>0.695771</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>The Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is Transformer?</td>\n",
       "      <td>0.679655</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is Transformer?</td>\n",
       "      <td>0.674317</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is Transformer?</td>\n",
       "      <td>0.668299</td>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>For example, the largest Transformer explored in\\nVaswani et al. (2017) is (L=6, H=1024, A=16)\\nwith 100M parameters for the encoder, and the\\nlargest Transformer we have found in the literature\\nis (L=64, H=512, A=2) with 235M parameters\\n(Al-Rfou et al., 2018).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What is Transformer?</td>\n",
       "      <td>0.662332</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What is Transformer?</td>\n",
       "      <td>0.657973</td>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>former is often referred to as a Transformer encoder while\\nthe left-context-only version is referred to as a Transformer\\ndecoder since it can be used for text generation.\\nIn order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               question    scores  \\\n",
       "0  What is Transformer?  0.696613   \n",
       "1  What is Transformer?  0.695771   \n",
       "2  What is Transformer?  0.679655   \n",
       "3  What is Transformer?  0.674317   \n",
       "4  What is Transformer?  0.668299   \n",
       "5  What is Transformer?  0.662332   \n",
       "6  What is Transformer?  0.657973   \n",
       "\n",
       "                                                                                             source  \\\n",
       "0                                                        attention is all you need 1706.03762v7.pdf   \n",
       "1                                                        attention is all you need 1706.03762v7.pdf   \n",
       "2                                                        attention is all you need 1706.03762v7.pdf   \n",
       "3                                                        attention is all you need 1706.03762v7.pdf   \n",
       "4  BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "5                                                        attention is all you need 1706.03762v7.pdf   \n",
       "6  BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                 text  \n",
       "0                                                                                                                                                 6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8  \n",
       "1                                                                                                                                                                                                                                                                    The Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.  \n",
       "2                                                    Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers.  \n",
       "3  End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution.  \n",
       "4                                                                                                                                                                                             For example, the largest Transformer explored in\\nVaswani et al. (2017) is (L=6, H=1024, A=16)\\nwith 100M parameters for the encoder, and the\\nlargest Transformer we have found in the literature\\nis (L=64, H=512, A=2) with 235M parameters\\n(Al-Rfou et al., 2018).  \n",
       "5                                                                     7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers.  \n",
       "6                                                                                                                   former is often referred to as a Transformer encoder while\\nthe left-context-only version is referred to as a Transformer\\ndecoder since it can be used for text generation.\\nIn order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens.  "
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df[query]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "dda91111-0a42-47a6-afcb-966564fa81aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>scores</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is Transformer?</td>\n",
       "      <td>0.696613</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is Transformer?</td>\n",
       "      <td>0.695771</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>The Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is Transformer?</td>\n",
       "      <td>0.679655</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is Transformer?</td>\n",
       "      <td>0.674317</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is Transformer?</td>\n",
       "      <td>0.668299</td>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>For example, the largest Transformer explored in\\nVaswani et al. (2017) is (L=6, H=1024, A=16)\\nwith 100M parameters for the encoder, and the\\nlargest Transformer we have found in the literature\\nis (L=64, H=512, A=2) with 235M parameters\\n(Al-Rfou et al., 2018).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What is Transformer?</td>\n",
       "      <td>0.662332</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What is Transformer?</td>\n",
       "      <td>0.657973</td>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>former is often referred to as a Transformer encoder while\\nthe left-context-only version is referred to as a Transformer\\ndecoder since it can be used for text generation.\\nIn order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is BERT?</td>\n",
       "      <td>0.811123</td>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>BERT is the rst ne-\\ntuning based representation model that achieves\\nstate-of-the-art performance on a large suite\\nof sentence-level and token-level tasks, outper-\\nforming many task-specic architectures.\\n BERT advances the state of the art for eleven\\nNLP tasks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is BERT?</td>\n",
       "      <td>0.751915</td>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>BERT is conceptually simple and empirically\\npowerful. It obtains new state-of-the-art re-\\nsults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is BERT?</td>\n",
       "      <td>0.751781</td>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>BERT BERT\\nE[CLS] E1  E[SEP]. EN E1 . EM\\nC\\n T1\\n T[SEP].\\n TN\\n T1 .\\n TM\\n[CLS] Tok 1  [SEP]. Tok N Tok 1 . TokM\\nQuestion Paragraph\\nStart/End Span\\nBERT\\nE[CLS] E1  E[SEP].</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is BERT?</td>\n",
       "      <td>0.740903</td>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>TokM\\nQuestion Paragraph\\nStart/End Span\\nBERT\\nE[CLS] E1  E[SEP]. EN E1 . EM\\nC\\n T1\\n T[SEP].\\n TN\\n T1 .\\n TM\\n[CLS] Tok 1  [SEP]. Tok N Tok 1 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is BERT?</td>\n",
       "      <td>0.738021</td>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>Input/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g., Question, Answer ) in one token sequence.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What is BERT?</td>\n",
       "      <td>0.733836</td>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>Each downstream task has sep-\\narate ne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\nA distinctive feature of BERT is its unied ar-\\nchitecture across different tasks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What is BERT?</td>\n",
       "      <td>0.731738</td>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>BERT\\nE[CLS] E1  E[SEP]. EN E1 . EM\\nC\\n T1\\n T[SEP].\\n TN\\n T1 .\\n TM\\n[CLS] Tok \\n1\\n [SEP]. Tok \\nN\\nTok \\n1 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is multi-head attention?</td>\n",
       "      <td>0.800710</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is multi-head attention?</td>\n",
       "      <td>0.784158</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is multi-head attention?</td>\n",
       "      <td>0.772050</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is multi-head attention?</td>\n",
       "      <td>0.758912</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\nwhere headi = Attention(QWQ\\ni , KWK\\ni , V WV\\ni )\\nWhere the projections are parameter matricesWQ\\ni  Rdmodeldk , WK\\ni  Rdmodeldk , WV\\ni  Rdmodeldv\\nand WO  Rhdvdmodel .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is multi-head attention?</td>\n",
       "      <td>0.740496</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [ 1].</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What is multi-head attention?</td>\n",
       "      <td>0.738943</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>To counteract this effect, we scale the dot products by 1dk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What is multi-head attention?</td>\n",
       "      <td>0.738749</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>In this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        question    scores  \\\n",
       "0           What is Transformer?  0.696613   \n",
       "1           What is Transformer?  0.695771   \n",
       "2           What is Transformer?  0.679655   \n",
       "3           What is Transformer?  0.674317   \n",
       "4           What is Transformer?  0.668299   \n",
       "5           What is Transformer?  0.662332   \n",
       "6           What is Transformer?  0.657973   \n",
       "0                  What is BERT?  0.811123   \n",
       "1                  What is BERT?  0.751915   \n",
       "2                  What is BERT?  0.751781   \n",
       "3                  What is BERT?  0.740903   \n",
       "4                  What is BERT?  0.738021   \n",
       "5                  What is BERT?  0.733836   \n",
       "6                  What is BERT?  0.731738   \n",
       "0  What is multi-head attention?  0.800710   \n",
       "1  What is multi-head attention?  0.784158   \n",
       "2  What is multi-head attention?  0.772050   \n",
       "3  What is multi-head attention?  0.758912   \n",
       "4  What is multi-head attention?  0.740496   \n",
       "5  What is multi-head attention?  0.738943   \n",
       "6  What is multi-head attention?  0.738749   \n",
       "\n",
       "                                                                                             source  \\\n",
       "0                                                        attention is all you need 1706.03762v7.pdf   \n",
       "1                                                        attention is all you need 1706.03762v7.pdf   \n",
       "2                                                        attention is all you need 1706.03762v7.pdf   \n",
       "3                                                        attention is all you need 1706.03762v7.pdf   \n",
       "4  BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "5                                                        attention is all you need 1706.03762v7.pdf   \n",
       "6  BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "0  BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "1  BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "2  BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "3  BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "4  BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "5  BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "6  BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "0                                                        attention is all you need 1706.03762v7.pdf   \n",
       "1                                                        attention is all you need 1706.03762v7.pdf   \n",
       "2                                                        attention is all you need 1706.03762v7.pdf   \n",
       "3                                                        attention is all you need 1706.03762v7.pdf   \n",
       "4                                                        attention is all you need 1706.03762v7.pdf   \n",
       "5                                                        attention is all you need 1706.03762v7.pdf   \n",
       "6                                                        attention is all you need 1706.03762v7.pdf   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                 text  \n",
       "0                                                                                                                                                 6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8  \n",
       "1                                                                                                                                                                                                                                                                    The Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.  \n",
       "2                                                    Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers.  \n",
       "3  End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution.  \n",
       "4                                                                                                                                                                                             For example, the largest Transformer explored in\\nVaswani et al. (2017) is (L=6, H=1024, A=16)\\nwith 100M parameters for the encoder, and the\\nlargest Transformer we have found in the literature\\nis (L=64, H=512, A=2) with 235M parameters\\n(Al-Rfou et al., 2018).  \n",
       "5                                                                     7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers.  \n",
       "6                                                                                                                   former is often referred to as a Transformer encoder while\\nthe left-context-only version is referred to as a Transformer\\ndecoder since it can be used for text generation.\\nIn order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens.  \n",
       "0                                                                                                                                                                                      BERT is the rst ne-\\ntuning based representation model that achieves\\nstate-of-the-art performance on a large suite\\nof sentence-level and token-level tasks, outper-\\nforming many task-specic architectures.\\n BERT advances the state of the art for eleven\\nNLP tasks.  \n",
       "1                                                                                                                                                                   BERT is conceptually simple and empirically\\npowerful. It obtains new state-of-the-art re-\\nsults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.  \n",
       "2                                                                                                                                                                                                                                                                               BERT BERT\\nE[CLS] E1  E[SEP]. EN E1 . EM\\nC\\n T1\\n T[SEP].\\n TN\\n T1 .\\n TM\\n[CLS] Tok 1  [SEP]. Tok N Tok 1 . TokM\\nQuestion Paragraph\\nStart/End Span\\nBERT\\nE[CLS] E1  E[SEP].  \n",
       "3                                                                                                                                                                                                                                                                                                             TokM\\nQuestion Paragraph\\nStart/End Span\\nBERT\\nE[CLS] E1  E[SEP]. EN E1 . EM\\nC\\n T1\\n T[SEP].\\n TN\\n T1 .\\n TM\\n[CLS] Tok 1  [SEP]. Tok N Tok 1 .  \n",
       "4                                                                                                                                                                                                                  Input/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g., Question, Answer ) in one token sequence.  \n",
       "5                                                                                                                                                Each downstream task has sep-\\narate ne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\nA distinctive feature of BERT is its unied ar-\\nchitecture across different tasks.  \n",
       "6                                                                                                                                                                                                                                                                                                                                               BERT\\nE[CLS] E1  E[SEP]. EN E1 . EM\\nC\\n T1\\n T[SEP].\\n TN\\n T1 .\\n TM\\n[CLS] Tok \\n1\\n [SEP]. Tok \\nN\\nTok \\n1 .  \n",
       "0                                                                                                                               Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.  \n",
       "1                                                                                    3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence.  \n",
       "2                                                                                                                                            output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.  \n",
       "3                                                                                                                                                                      With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\nwhere headi = Attention(QWQ\\ni , KWK\\ni , V WV\\ni )\\nWhere the projections are parameter matricesWQ\\ni  Rdmodeldk , WK\\ni  Rdmodeldk , WV\\ni  Rdmodeldv\\nand WO  Rhdvdmodel .  \n",
       "4                                                                                                                                                                                Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [ 1].  \n",
       "5                                                                                            To counteract this effect, we scale the dot products by 1dk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively.  \n",
       "6                                                                                                                                                                                                 In this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.  "
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = pd.concat([output_df[key] for key in output_df])\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "4ee12b58-e26e-49dc-ba14-c6be147f5553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['index', 'question', 'scores', 'source', 'text']\n"
     ]
    }
   ],
   "source": [
    "merged_df = merged_df.reset_index()\n",
    "cols = list(merged_df.columns)\n",
    "print(cols)\n",
    "cols.remove('index')\n",
    "merged_df = merged_df.groupby(['question','source'])[cols].apply(lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "d71edef6-f216-4754-b313-7c328188e7f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>scores</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question</th>\n",
       "      <th>source</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">What is BERT?</th>\n",
       "      <th rowspan=\"7\" valign=\"top\">BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</th>\n",
       "      <th>7</th>\n",
       "      <td>What is BERT?</td>\n",
       "      <td>0.811123</td>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>BERT is the rst ne-\\ntuning based representation model that achieves\\nstate-of-the-art performance on a large suite\\nof sentence-level and token-level tasks, outper-\\nforming many task-specic architectures.\\n BERT advances the state of the art for eleven\\nNLP tasks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What is BERT?</td>\n",
       "      <td>0.751915</td>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>BERT is conceptually simple and empirically\\npowerful. It obtains new state-of-the-art re-\\nsults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What is BERT?</td>\n",
       "      <td>0.751781</td>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>BERT BERT\\nE[CLS] E1  E[SEP]. EN E1 . EM\\nC\\n T1\\n T[SEP].\\n TN\\n T1 .\\n TM\\n[CLS] Tok 1  [SEP]. Tok N Tok 1 . TokM\\nQuestion Paragraph\\nStart/End Span\\nBERT\\nE[CLS] E1  E[SEP].</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What is BERT?</td>\n",
       "      <td>0.740903</td>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>TokM\\nQuestion Paragraph\\nStart/End Span\\nBERT\\nE[CLS] E1  E[SEP]. EN E1 . EM\\nC\\n T1\\n T[SEP].\\n TN\\n T1 .\\n TM\\n[CLS] Tok 1  [SEP]. Tok N Tok 1 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>What is BERT?</td>\n",
       "      <td>0.738021</td>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>Input/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g., Question, Answer ) in one token sequence.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>What is BERT?</td>\n",
       "      <td>0.733836</td>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>Each downstream task has sep-\\narate ne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\nA distinctive feature of BERT is its unied ar-\\nchitecture across different tasks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>What is BERT?</td>\n",
       "      <td>0.731738</td>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>BERT\\nE[CLS] E1  E[SEP]. EN E1 . EM\\nC\\n T1\\n T[SEP].\\n TN\\n T1 .\\n TM\\n[CLS] Tok \\n1\\n [SEP]. Tok \\nN\\nTok \\n1 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">What is Transformer?</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</th>\n",
       "      <th>4</th>\n",
       "      <td>What is Transformer?</td>\n",
       "      <td>0.668299</td>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>For example, the largest Transformer explored in\\nVaswani et al. (2017) is (L=6, H=1024, A=16)\\nwith 100M parameters for the encoder, and the\\nlargest Transformer we have found in the literature\\nis (L=64, H=512, A=2) with 235M parameters\\n(Al-Rfou et al., 2018).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What is Transformer?</td>\n",
       "      <td>0.657973</td>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>former is often referred to as a Transformer encoder while\\nthe left-context-only version is referred to as a Transformer\\ndecoder since it can be used for text generation.\\nIn order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">attention is all you need 1706.03762v7.pdf</th>\n",
       "      <th>0</th>\n",
       "      <td>What is Transformer?</td>\n",
       "      <td>0.696613</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is Transformer?</td>\n",
       "      <td>0.695771</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>The Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is Transformer?</td>\n",
       "      <td>0.679655</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is Transformer?</td>\n",
       "      <td>0.674317</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What is Transformer?</td>\n",
       "      <td>0.662332</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">What is multi-head attention?</th>\n",
       "      <th rowspan=\"7\" valign=\"top\">attention is all you need 1706.03762v7.pdf</th>\n",
       "      <th>14</th>\n",
       "      <td>What is multi-head attention?</td>\n",
       "      <td>0.800710</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>What is multi-head attention?</td>\n",
       "      <td>0.784158</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>What is multi-head attention?</td>\n",
       "      <td>0.772050</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>What is multi-head attention?</td>\n",
       "      <td>0.758912</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\nwhere headi = Attention(QWQ\\ni , KWK\\ni , V WV\\ni )\\nWhere the projections are parameter matricesWQ\\ni  Rdmodeldk , WK\\ni  Rdmodeldk , WV\\ni  Rdmodeldv\\nand WO  Rhdvdmodel .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>What is multi-head attention?</td>\n",
       "      <td>0.740496</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [ 1].</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>What is multi-head attention?</td>\n",
       "      <td>0.738943</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>To counteract this effect, we scale the dot products by 1dk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>What is multi-head attention?</td>\n",
       "      <td>0.738749</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>In this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                        question  \\\n",
       "question                      source                                                                                                                               \n",
       "What is BERT?                 BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf 7                   What is BERT?   \n",
       "                                                                                                                               8                   What is BERT?   \n",
       "                                                                                                                               9                   What is BERT?   \n",
       "                                                                                                                               10                  What is BERT?   \n",
       "                                                                                                                               11                  What is BERT?   \n",
       "                                                                                                                               12                  What is BERT?   \n",
       "                                                                                                                               13                  What is BERT?   \n",
       "What is Transformer?          BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf 4            What is Transformer?   \n",
       "                                                                                                                               6            What is Transformer?   \n",
       "                              attention is all you need 1706.03762v7.pdf                                                       0            What is Transformer?   \n",
       "                                                                                                                               1            What is Transformer?   \n",
       "                                                                                                                               2            What is Transformer?   \n",
       "                                                                                                                               3            What is Transformer?   \n",
       "                                                                                                                               5            What is Transformer?   \n",
       "What is multi-head attention? attention is all you need 1706.03762v7.pdf                                                       14  What is multi-head attention?   \n",
       "                                                                                                                               15  What is multi-head attention?   \n",
       "                                                                                                                               16  What is multi-head attention?   \n",
       "                                                                                                                               17  What is multi-head attention?   \n",
       "                                                                                                                               18  What is multi-head attention?   \n",
       "                                                                                                                               19  What is multi-head attention?   \n",
       "                                                                                                                               20  What is multi-head attention?   \n",
       "\n",
       "                                                                                                                                     scores  \\\n",
       "question                      source                                                                                                          \n",
       "What is BERT?                 BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf 7   0.811123   \n",
       "                                                                                                                               8   0.751915   \n",
       "                                                                                                                               9   0.751781   \n",
       "                                                                                                                               10  0.740903   \n",
       "                                                                                                                               11  0.738021   \n",
       "                                                                                                                               12  0.733836   \n",
       "                                                                                                                               13  0.731738   \n",
       "What is Transformer?          BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf 4   0.668299   \n",
       "                                                                                                                               6   0.657973   \n",
       "                              attention is all you need 1706.03762v7.pdf                                                       0   0.696613   \n",
       "                                                                                                                               1   0.695771   \n",
       "                                                                                                                               2   0.679655   \n",
       "                                                                                                                               3   0.674317   \n",
       "                                                                                                                               5   0.662332   \n",
       "What is multi-head attention? attention is all you need 1706.03762v7.pdf                                                       14  0.800710   \n",
       "                                                                                                                               15  0.784158   \n",
       "                                                                                                                               16  0.772050   \n",
       "                                                                                                                               17  0.758912   \n",
       "                                                                                                                               18  0.740496   \n",
       "                                                                                                                               19  0.738943   \n",
       "                                                                                                                               20  0.738749   \n",
       "\n",
       "                                                                                                                                                                                                                             source  \\\n",
       "question                      source                                                                                                                                                                                                  \n",
       "What is BERT?                 BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf 7   BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "                                                                                                                               8   BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "                                                                                                                               9   BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "                                                                                                                               10  BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "                                                                                                                               11  BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "                                                                                                                               12  BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "                                                                                                                               13  BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "What is Transformer?          BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf 4   BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "                                                                                                                               6   BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "                              attention is all you need 1706.03762v7.pdf                                                       0                                                         attention is all you need 1706.03762v7.pdf   \n",
       "                                                                                                                               1                                                         attention is all you need 1706.03762v7.pdf   \n",
       "                                                                                                                               2                                                         attention is all you need 1706.03762v7.pdf   \n",
       "                                                                                                                               3                                                         attention is all you need 1706.03762v7.pdf   \n",
       "                                                                                                                               5                                                         attention is all you need 1706.03762v7.pdf   \n",
       "What is multi-head attention? attention is all you need 1706.03762v7.pdf                                                       14                                                        attention is all you need 1706.03762v7.pdf   \n",
       "                                                                                                                               15                                                        attention is all you need 1706.03762v7.pdf   \n",
       "                                                                                                                               16                                                        attention is all you need 1706.03762v7.pdf   \n",
       "                                                                                                                               17                                                        attention is all you need 1706.03762v7.pdf   \n",
       "                                                                                                                               18                                                        attention is all you need 1706.03762v7.pdf   \n",
       "                                                                                                                               19                                                        attention is all you need 1706.03762v7.pdf   \n",
       "                                                                                                                               20                                                        attention is all you need 1706.03762v7.pdf   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 text  \n",
       "question                      source                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "What is BERT?                 BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf 7                                                                                                                                                                                       BERT is the rst ne-\\ntuning based representation model that achieves\\nstate-of-the-art performance on a large suite\\nof sentence-level and token-level tasks, outper-\\nforming many task-specic architectures.\\n BERT advances the state of the art for eleven\\nNLP tasks.  \n",
       "                                                                                                                               8                                                                                                                                                                    BERT is conceptually simple and empirically\\npowerful. It obtains new state-of-the-art re-\\nsults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.  \n",
       "                                                                                                                               9                                                                                                                                                                                                                                                                                BERT BERT\\nE[CLS] E1  E[SEP]. EN E1 . EM\\nC\\n T1\\n T[SEP].\\n TN\\n T1 .\\n TM\\n[CLS] Tok 1  [SEP]. Tok N Tok 1 . TokM\\nQuestion Paragraph\\nStart/End Span\\nBERT\\nE[CLS] E1  E[SEP].  \n",
       "                                                                                                                               10                                                                                                                                                                                                                                                                                                             TokM\\nQuestion Paragraph\\nStart/End Span\\nBERT\\nE[CLS] E1  E[SEP]. EN E1 . EM\\nC\\n T1\\n T[SEP].\\n TN\\n T1 .\\n TM\\n[CLS] Tok 1  [SEP]. Tok N Tok 1 .  \n",
       "                                                                                                                               11                                                                                                                                                                                                                  Input/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g., Question, Answer ) in one token sequence.  \n",
       "                                                                                                                               12                                                                                                                                                Each downstream task has sep-\\narate ne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\nA distinctive feature of BERT is its unied ar-\\nchitecture across different tasks.  \n",
       "                                                                                                                               13                                                                                                                                                                                                                                                                                                                                               BERT\\nE[CLS] E1  E[SEP]. EN E1 . EM\\nC\\n T1\\n T[SEP].\\n TN\\n T1 .\\n TM\\n[CLS] Tok \\n1\\n [SEP]. Tok \\nN\\nTok \\n1 .  \n",
       "What is Transformer?          BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf 4                                                                                                                                                                                              For example, the largest Transformer explored in\\nVaswani et al. (2017) is (L=6, H=1024, A=16)\\nwith 100M parameters for the encoder, and the\\nlargest Transformer we have found in the literature\\nis (L=64, H=512, A=2) with 235M parameters\\n(Al-Rfou et al., 2018).  \n",
       "                                                                                                                               6                                                                                                                    former is often referred to as a Transformer encoder while\\nthe left-context-only version is referred to as a Transformer\\ndecoder since it can be used for text generation.\\nIn order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens.  \n",
       "                              attention is all you need 1706.03762v7.pdf                                                       0                                                                                                                                                  6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8  \n",
       "                                                                                                                               1                                                                                                                                                                                                                                                                     The Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.  \n",
       "                                                                                                                               2                                                     Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers.  \n",
       "                                                                                                                               3   End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution.  \n",
       "                                                                                                                               5                                                                      7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers.  \n",
       "What is multi-head attention? attention is all you need 1706.03762v7.pdf                                                       14                                                                                                                               Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.  \n",
       "                                                                                                                               15                                                                                    3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence.  \n",
       "                                                                                                                               16                                                                                                                                            output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.  \n",
       "                                                                                                                               17                                                                                                                                                                      With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\nwhere headi = Attention(QWQ\\ni , KWK\\ni , V WV\\ni )\\nWhere the projections are parameter matricesWQ\\ni  Rdmodeldk , WK\\ni  Rdmodeldk , WV\\ni  Rdmodeldv\\nand WO  Rhdvdmodel .  \n",
       "                                                                                                                               18                                                                                                                                                                                Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [ 1].  \n",
       "                                                                                                                               19                                                                                            To counteract this effect, we scale the dot products by 1dk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively.  \n",
       "                                                                                                                               20                                                                                                                                                                                                 In this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.  "
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
