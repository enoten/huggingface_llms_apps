{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5065cbca-a35b-4028-85f6-923d81f347ca",
   "metadata": {},
   "source": [
    "<img src=\"ask questions to LLMs what.jpg\" width=70%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ee08b4-1d89-405c-9e6d-8bb25b6e59d1",
   "metadata": {},
   "source": [
    "## How to Ask Questions to LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833b2bfd-65c3-4774-b967-2d17ef684ef5",
   "metadata": {},
   "source": [
    "<h3>Code walk through video <a href=\"https://www.youtube.com/111\">How to Ask Questions to LLMs</a></b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab13400-3547-490d-bb6e-9822559418e0",
   "metadata": {},
   "source": [
    "<a href=\"https://arxiv.org/pdf/1706.03762\">Attention is all you need</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef18776-e541-4c67-8211-5ae638941a87",
   "metadata": {},
   "source": [
    "<img src=\"attention.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba624af-08c6-4a26-8907-404caba2b7bd",
   "metadata": {},
   "source": [
    "<a href=\"https://arxiv.org/pdf/1810.04805\">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a6caf3-404a-414f-a090-4d5b309f463b",
   "metadata": {},
   "source": [
    "<img src=\"bert.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0c4a91-3191-4477-b858-aac9213f9fcf",
   "metadata": {},
   "source": [
    "## 1. Read PDF files with research papers using Llama-Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d96e764e-ba80-4ba2-8b4d-5b54463e29b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d00949b-2546-41bc-9eaa-da3487025fb9",
   "metadata": {},
   "source": [
    "<img src=\"LlamaIndex_schema.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4f578bb7-7f8e-48ab-ac8e-af26a16aaf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1c69ab-b754-4d7d-8c21-bc39ec737fe7",
   "metadata": {},
   "source": [
    "<b>1.1 Setup model for embeddings</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c478cf42-6550-4cd3-8a3c-c42f24dfc451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    }
   ],
   "source": [
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "Settings.llm = None\n",
    "Settings.chunk_size = 128\n",
    "Settings.chunk_overlap = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b37a1e-4cbd-49c9-b130-8b9b14d1aa4b",
   "metadata": {},
   "source": [
    "<b>1.2 Read files one by one from the directory</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a6ba0322-49db-4c51-b1e6-29d80e890958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fs': <fsspec.implementations.local.LocalFileSystem at 0x25938c1fd60>,\n",
       " 'errors': 'ignore',\n",
       " 'encoding': 'utf-8',\n",
       " 'exclude': None,\n",
       " 'recursive': True,\n",
       " 'exclude_hidden': True,\n",
       " 'required_exts': None,\n",
       " 'num_files_limit': None,\n",
       " 'raise_on_error': False,\n",
       " 'input_dir': WindowsPath('papers'),\n",
       " 'input_files': [WindowsPath('C:/Users/18623/Desktop/PhiAi/Jupyter/papers/attention is all you need 1706.03762v7.pdf'),\n",
       "  WindowsPath('C:/Users/18623/Desktop/PhiAi/Jupyter/papers/BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf')],\n",
       " 'file_extractor': {},\n",
       " 'file_metadata': <llama_index.core.readers.file.base._DefaultFileMetadataFunc at 0x25a26555e50>,\n",
       " 'filename_as_id': False}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader = SimpleDirectoryReader(input_dir=\"papers\", recursive=True)\n",
    "reader.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9c536b42-253f-4146-a0f7-d61471a2cd83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('C:/Users/18623/Desktop/PhiAi/Jupyter/papers/attention is all you need 1706.03762v7.pdf'),\n",
       " WindowsPath('C:/Users/18623/Desktop/PhiAi/Jupyter/papers/BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf')]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader.__dict__['input_files']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "90dba2cd-8c95-4e30-8787-50f96944d61e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('C:/Users/18623/Desktop/PhiAi/Jupyter/papers/attention is all you need 1706.03762v7.pdf'),\n",
       " WindowsPath('C:/Users/18623/Desktop/PhiAi/Jupyter/papers/BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf')]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader.input_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cd910b-9b01-4808-814a-189a964c18d5",
   "metadata": {},
   "source": [
    "<b>1.3 Store Text chunks in a dict to trace chunk-paper mapping</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f103d12c-8486-428d-bfb6-362c3d1ab93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract from file attention is all you need 1706.03762v7.pdf\n",
      "Extract from file BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf\n"
     ]
    }
   ],
   "source": [
    "pdf_files = reader.__dict__['input_files']\n",
    "all_docs = []\n",
    "doc_dict = {}\n",
    "for i,docs in enumerate(reader.iter_data()):\n",
    "    pdf_name = os.path.basename(pdf_files[i])\n",
    "    print(f'Extract from file {pdf_name}')\n",
    "    if pdf_name in doc_dict:\n",
    "        doc_dict[pdf_name] += [docs]\n",
    "    else:\n",
    "        doc_dict[pdf_name] = [docs]\n",
    "    all_docs.extend(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c40a926-8a10-4689-9562-1421eb3bf2fd",
   "metadata": {},
   "source": [
    "<b>1.4 Inspect Llama-Index document class</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e193598e-04e6-4312-94ed-f7ea50956e12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['attention is all you need 1706.03762v7.pdf', 'BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0fb0fba0-47f3-4bea-b48e-573b0fe53648",
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7b3de327-5a1e-4eb4-baea-99f2ef2c4bb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_dict['attention is all you need 1706.03762v7.pdf'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "95ffcf45-5313-484f-a3b7-f6e45ac56e1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_dict['BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ab3b9d45-7013-4144-89f6-0555afb9fac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = doc_dict['attention is all you need 1706.03762v7.pdf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "54909f3f-6419-474a-a5b6-2de317684b00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id_': '757dfa3d-eac9-419b-b818-8d82813df6c6',\n",
       " 'embedding': None,\n",
       " 'metadata': {'page_label': '1',\n",
       "  'file_name': 'attention is all you need 1706.03762v7.pdf',\n",
       "  'file_path': 'C:\\\\Users\\\\18623\\\\Desktop\\\\PhiAi\\\\Jupyter\\\\papers\\\\attention is all you need 1706.03762v7.pdf',\n",
       "  'file_type': 'application/pdf',\n",
       "  'file_size': 2215244,\n",
       "  'creation_date': '2024-12-11',\n",
       "  'last_modified_date': '2024-12-11'},\n",
       " 'excluded_embed_metadata_keys': ['file_name',\n",
       "  'file_type',\n",
       "  'file_size',\n",
       "  'creation_date',\n",
       "  'last_modified_date',\n",
       "  'last_accessed_date'],\n",
       " 'excluded_llm_metadata_keys': ['file_name',\n",
       "  'file_type',\n",
       "  'file_size',\n",
       "  'creation_date',\n",
       "  'last_modified_date',\n",
       "  'last_accessed_date'],\n",
       " 'relationships': {},\n",
       " 'text': 'Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023',\n",
       " 'mimetype': 'text/plain',\n",
       " 'start_char_idx': None,\n",
       " 'end_char_idx': None,\n",
       " 'text_template': '{metadata_str}\\n\\n{content}',\n",
       " 'metadata_template': '{key}: {value}',\n",
       " 'metadata_seperator': '\\n'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0][0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "856043cd-879f-494e-bb18-0110eb53a819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'757dfa3d-eac9-419b-b818-8d82813df6c6'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0][0].__dict__['id_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ab1da91-fb67-411b-88a7-5eaad6a4899a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'757dfa3d-eac9-419b-b818-8d82813df6c6'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0][0].id_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326b8a18-91a4-4a32-9739-93ba1a963d23",
   "metadata": {},
   "source": [
    "<b>1.5 Get total number of collected chunks</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "366114ae-55ab-4232-b16b-dcf6ea1c7054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca8b6ea-ae32-498f-8e75-1a683d2135c3",
   "metadata": {},
   "source": [
    "<b>1.6 Get chuck distribution accross papers</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dde5e148-5fce-4f61-8dc4-cf1296e93b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention is all you need 1706.03762v7.pdf >>> 1 chunks\n",
      "BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf >>> 1 chunks\n"
     ]
    }
   ],
   "source": [
    "for key in doc_dict:\n",
    "    num_chunks = len(doc_dict[key])\n",
    "    print(f'{key} >>> {num_chunks} chunks')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a672c9-3502-43c9-b213-30bb1cad9c6e",
   "metadata": {},
   "source": [
    "<b>1.7 Collect all chunks for each paper in one text</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "297e44ba-86fe-4058-ba90-e2c1bd33d35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = list(doc_dict.keys())\n",
    "papers = {} \n",
    "\n",
    "for paper_name in articles:\n",
    "    papers[paper_name] = ''\n",
    "    for doc in doc_dict[paper_name]:\n",
    "        papers[paper_name] += ' ' + doc[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "433a58a7-8658-414d-8c53-55fa7b0fcf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "for paper_name in articles:\n",
    "    papers[paper_name] = papers[paper_name].replace('.\\n','. ')\n",
    "    papers[paper_name] = papers[paper_name].replace('\\n',' ')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f2c88f24-36d3-4fc2-8f3c-81f2f490dbd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works. Attention Is All You Need Ashish Vaswani∗ Google Brain avaswani@google.com Noam Shazeer∗ Google Brain noam@google.com Niki Parmar∗ Google Research nikip@google.com Jakob Uszkoreit∗ Google Research usz@google.com Llion Jones∗ Google Research llion@google.com Aidan N. Gomez∗ † University of Toronto aidan@cs.toronto.edu Łukasz Kaiser∗ Google Brain lukaszkaiser@google.com Illia Polosukhin∗ ‡ illia.polosukhin@gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. ∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research. †Work performed while at Google Brain. ‡Work performed while at Google Research. 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA. arXiv:1706.03762v7  [cs.CL]  2 Aug 2023'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers[articles[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ea3e6d-38f5-49ee-8544-3d86461ce15d",
   "metadata": {},
   "source": [
    "## 2. QA: Ask questions about the papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6060b5-9d97-4b74-a2cb-9197e7232a33",
   "metadata": {},
   "source": [
    "<b>2.1 Roberta-base-sqaud2-distilled model</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d7c1ea97-4f7f-4a06-a074-e964be5f9afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d0e93c3-c3d5-4eec-9773-90fdd155d303",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1489042f-95f5-4c91-8661-771912e01101",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"deepset/roberta-base-squad2-distilled\"\n",
    "\n",
    "# a) Get predictions\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "nlp = pipeline('question-answering', \n",
    "               model=model_name, \n",
    "               tokenizer=model_name,\n",
    "               device = device\n",
    "              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9e24d267-2de0-45fb-b35f-55529428da0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\18623\\anaconda3\\envs\\transformers\\lib\\site-packages\\transformers\\pipelines\\question_answering.py:391: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\18623\\anaconda3\\envs\\transformers\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:370: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>model name</th>\n",
       "      <th>paper</th>\n",
       "      <th>answer</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what is attention?</td>\n",
       "      <td>deepset/roberta-base-squad2-distilled</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>All You Need</td>\n",
       "      <td>0.038977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what is attention?</td>\n",
       "      <td>deepset/roberta-base-squad2-distilled</td>\n",
       "      <td>BERT pre_training of deep bidirectional transf...</td>\n",
       "      <td>conceptually simple and empirically powerful</td>\n",
       "      <td>0.000194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             question                             model name  \\\n",
       "0  what is attention?  deepset/roberta-base-squad2-distilled   \n",
       "1  what is attention?  deepset/roberta-base-squad2-distilled   \n",
       "\n",
       "                                               paper  \\\n",
       "0         attention is all you need 1706.03762v7.pdf   \n",
       "1  BERT pre_training of deep bidirectional transf...   \n",
       "\n",
       "                                         answer     score  \n",
       "0                                  All You Need  0.038977  \n",
       "1  conceptually simple and empirically powerful  0.000194  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_list = []\n",
    "score_list = []\n",
    "for paper in articles:\n",
    "    question = 'what is attention?'\n",
    "    QA_input = {\n",
    "        'question': question,\n",
    "        'context': papers[paper]\n",
    "    }\n",
    "    res = nlp(QA_input)\n",
    "    #print(f'from paper {paper} \\n we learn that answer to {question} is \\n {res} \\n')\n",
    "    answer_list.append(res['answer'])\n",
    "    score_list.append(res['score'])\n",
    "\n",
    "dfs[question + model_name] = pd.DataFrame({'question':len(articles)*[question],\n",
    "                                           'model name':len(articles)*[model_name],\n",
    "                                           'paper':articles,\n",
    "                                           'answer': answer_list,\n",
    "                                           'score':score_list\n",
    "                             })\n",
    "dfs[question + model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fc51a280-dfc0-47cb-a549-db721ce43304",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fcee0bdb-542d-4b05-8286-29a6cac9cd4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>model name</th>\n",
       "      <th>paper</th>\n",
       "      <th>answer</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what is attention?</td>\n",
       "      <td>deepset/roberta-base-squad2-distilled</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>All You Need</td>\n",
       "      <td>0.038977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what is attention?</td>\n",
       "      <td>deepset/roberta-base-squad2-distilled</td>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>conceptually simple and empirically powerful</td>\n",
       "      <td>0.000194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             question                             model name  \\\n",
       "0  what is attention?  deepset/roberta-base-squad2-distilled   \n",
       "1  what is attention?  deepset/roberta-base-squad2-distilled   \n",
       "\n",
       "                                                                                              paper  \\\n",
       "0                                                        attention is all you need 1706.03762v7.pdf   \n",
       "1  BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "\n",
       "                                         answer     score  \n",
       "0                                  All You Need  0.038977  \n",
       "1  conceptually simple and empirically powerful  0.000194  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[question + model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "519796ab-d401-4869-95c0-7b9307cff398",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\18623\\anaconda3\\envs\\transformers\\lib\\site-packages\\transformers\\pipelines\\question_answering.py:391: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>model name</th>\n",
       "      <th>paper</th>\n",
       "      <th>answer</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what types of attention are defined?</td>\n",
       "      <td>deepset/roberta-base-squad2-distilled</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>multi-head attention</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what types of attention are defined?</td>\n",
       "      <td>deepset/roberta-base-squad2-distilled</td>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>self-attention layers</td>\n",
       "      <td>0.000024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               question  \\\n",
       "0  what types of attention are defined?   \n",
       "1  what types of attention are defined?   \n",
       "\n",
       "                              model name  \\\n",
       "0  deepset/roberta-base-squad2-distilled   \n",
       "1  deepset/roberta-base-squad2-distilled   \n",
       "\n",
       "                                                                                              paper  \\\n",
       "0                                                        attention is all you need 1706.03762v7.pdf   \n",
       "1  BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "\n",
       "                  answer     score  \n",
       "0   multi-head attention  0.000005  \n",
       "1  self-attention layers  0.000024  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_list = []\n",
    "score_list = []\n",
    "for paper in articles:\n",
    "    question = 'what types of attention are defined?'\n",
    "    QA_input = {\n",
    "        'question': question,\n",
    "        'context': papers[paper]\n",
    "    }\n",
    "    res = nlp(QA_input)\n",
    "    #print(f'from paper {paper} \\n we learn that answer to {question} is \\n {res} \\n')\n",
    "    answer_list.append(res['answer'])\n",
    "    score_list.append(res['score'])\n",
    "\n",
    "dfs[question + model_name] = pd.DataFrame({'question':len(articles)*[question],\n",
    "                              'model name':len(articles)*[model_name],\n",
    "                              'paper':articles,\n",
    "                              'answer': answer_list,\n",
    "                              'score':score_list\n",
    "                             })\n",
    "dfs[question + model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "55a5c48f-782a-4ced-8d01-2bac57bff342",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\18623\\anaconda3\\envs\\transformers\\lib\\site-packages\\transformers\\pipelines\\question_answering.py:391: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>model name</th>\n",
       "      <th>paper</th>\n",
       "      <th>answer</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what is multi-head attention?</td>\n",
       "      <td>deepset/roberta-base-squad2-distilled</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>parameter-free position representation</td>\n",
       "      <td>0.003145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what is multi-head attention?</td>\n",
       "      <td>deepset/roberta-base-squad2-distilled</td>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>MultiNLI accuracy</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        question                             model name  \\\n",
       "0  what is multi-head attention?  deepset/roberta-base-squad2-distilled   \n",
       "1  what is multi-head attention?  deepset/roberta-base-squad2-distilled   \n",
       "\n",
       "                                                                                              paper  \\\n",
       "0                                                        attention is all you need 1706.03762v7.pdf   \n",
       "1  BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "\n",
       "                                   answer     score  \n",
       "0  parameter-free position representation  0.003145  \n",
       "1                       MultiNLI accuracy  0.000003  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_list = []\n",
    "score_list = []\n",
    "for paper in articles:\n",
    "    question = 'what is multi-head attention?'\n",
    "    QA_input = {\n",
    "        'question': question,\n",
    "        'context': papers[paper]\n",
    "    }\n",
    "    res = nlp(QA_input)\n",
    "    #print(f'from paper {paper} \\n we learn that answer to {question} is \\n {res} \\n')\n",
    "    answer_list.append(res['answer'])\n",
    "    score_list.append(res['score'])\n",
    "\n",
    "dfs[question + model_name] = pd.DataFrame({'question':len(articles)*[question],\n",
    "                              'model name':len(articles)*[model_name],\n",
    "                              'paper':articles,\n",
    "                              'answer': answer_list,\n",
    "                              'score':score_list\n",
    "                             })\n",
    "dfs[question + model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f24ad9-b99b-47ed-97e6-a2d4c2efdd66",
   "metadata": {},
   "source": [
    "<b>2.2 HuggingFace examplar pipeline</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a6eb06fc-59bd-4e7c-91b4-c16fec3c215d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "qa_model = pipeline(\"question-answering\", \n",
    "                    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d994b336-ee03-48b7-90ce-7f62e667a4b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>model name</th>\n",
       "      <th>paper</th>\n",
       "      <th>answer</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what is attention?</td>\n",
       "      <td>default</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>multi-head</td>\n",
       "      <td>0.680392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what is attention?</td>\n",
       "      <td>default</td>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>improving many natural language</td>\n",
       "      <td>0.133738</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             question model name  \\\n",
       "0  what is attention?    default   \n",
       "1  what is attention?    default   \n",
       "\n",
       "                                                                                              paper  \\\n",
       "0                                                        attention is all you need 1706.03762v7.pdf   \n",
       "1  BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "\n",
       "                            answer     score  \n",
       "0                       multi-head  0.680392  \n",
       "1  improving many natural language  0.133738  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_list = []\n",
    "score_list = []\n",
    "for paper in articles:\n",
    "    question = 'what is attention?'\n",
    "    res = qa_model(question = question, context = papers[paper])\n",
    "    #print(f'from paper {paper} \\n we learn that answer to {question} is \\n {res} \\n')\n",
    "    answer_list.append(res['answer'])\n",
    "    score_list.append(res['score'])\n",
    "\n",
    "dfs[question+'_nomodel'] = pd.DataFrame({'question':len(articles)*[question],\n",
    "                                         'model name': len(articles)*['default'],\n",
    "                                         'paper':articles,\n",
    "                                         'answer': answer_list,\n",
    "                                         'score':score_list\n",
    "                                        })\n",
    "dfs[question+'_nomodel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "42d94006-22f5-4efb-b245-0c39ee9b9ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>model name</th>\n",
       "      <th>paper</th>\n",
       "      <th>answer</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what types of attention are defined?</td>\n",
       "      <td>default</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>multi-head</td>\n",
       "      <td>0.289631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what types of attention are defined?</td>\n",
       "      <td>default</td>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>deep bidirectional representations</td>\n",
       "      <td>0.003118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               question model name  \\\n",
       "0  what types of attention are defined?    default   \n",
       "1  what types of attention are defined?    default   \n",
       "\n",
       "                                                                                              paper  \\\n",
       "0                                                        attention is all you need 1706.03762v7.pdf   \n",
       "1  BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "\n",
       "                               answer     score  \n",
       "0                          multi-head  0.289631  \n",
       "1  deep bidirectional representations  0.003118  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_list = []\n",
    "score_list = []\n",
    "for paper in articles:\n",
    "    question = 'what types of attention are defined?'\n",
    "    res = qa_model(question = question, context = papers[paper])\n",
    "    #print(f'from paper {paper} \\n we learn that answer to {question} is \\n {res} \\n')\n",
    "    answer_list.append(res['answer'])\n",
    "    score_list.append(res['score'])\n",
    "\n",
    "dfs[question+'_nomodel'] = pd.DataFrame({'question':len(articles)*[question],\n",
    "                                         'model name': len(articles)*['default'],\n",
    "                                         'paper':articles,\n",
    "                                         'answer': answer_list,\n",
    "                                         'score':score_list\n",
    "                                        })\n",
    "dfs[question+'_nomodel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5d7cd3bf-907b-4e04-8708-c0278ca42eb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>model name</th>\n",
       "      <th>paper</th>\n",
       "      <th>answer</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what is multi-head attention?</td>\n",
       "      <td>default</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>scaled dot-product attention</td>\n",
       "      <td>0.802400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what is multi-head attention?</td>\n",
       "      <td>default</td>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>Language</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        question model name  \\\n",
       "0  what is multi-head attention?    default   \n",
       "1  what is multi-head attention?    default   \n",
       "\n",
       "                                                                                              paper  \\\n",
       "0                                                        attention is all you need 1706.03762v7.pdf   \n",
       "1  BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "\n",
       "                         answer     score  \n",
       "0  scaled dot-product attention  0.802400  \n",
       "1                      Language  0.000012  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_list = []\n",
    "score_list = []\n",
    "for paper in articles:\n",
    "    question = 'what is multi-head attention?'\n",
    "    res = qa_model(question = question, context = papers[paper])\n",
    "    #print(f'from paper {paper} \\n we learn that answer to {question} is \\n {res} \\n')\n",
    "    answer_list.append(res['answer'])\n",
    "    score_list.append(res['score'])\n",
    "\n",
    "dfs[question+'_nomodel'] = pd.DataFrame({'question':len(articles)*[question],\n",
    "                                         'model name': len(articles)*['default'],\n",
    "                                         'paper':articles,\n",
    "                                         'answer': answer_list,\n",
    "                                         'score':score_list\n",
    "                                        })\n",
    "dfs[question+'_nomodel']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda20157-4d5d-4520-bfd9-0376b449031e",
   "metadata": {},
   "source": [
    "<b>2.3 Summarize QA results in one DataFrame</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "af28b227-e490-4a2a-b900-094aca62ba66",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_keys = list(dfs.keys())\n",
    "\n",
    "merged_df = pd.concat([dfs[key] for key in qa_keys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "132b54f6-8491-4401-8562-2bca226c2cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>model name</th>\n",
       "      <th>paper</th>\n",
       "      <th>answer</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what is attention?</td>\n",
       "      <td>deepset/roberta-base-squad2-distilled</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>All You Need</td>\n",
       "      <td>0.038977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what is attention?</td>\n",
       "      <td>deepset/roberta-base-squad2-distilled</td>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>conceptually simple and empirically powerful</td>\n",
       "      <td>0.000194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what types of attention are defined?</td>\n",
       "      <td>deepset/roberta-base-squad2-distilled</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>multi-head attention</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what types of attention are defined?</td>\n",
       "      <td>deepset/roberta-base-squad2-distilled</td>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>self-attention layers</td>\n",
       "      <td>0.000024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what is multi-head attention?</td>\n",
       "      <td>deepset/roberta-base-squad2-distilled</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>parameter-free position representation</td>\n",
       "      <td>0.003145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what is multi-head attention?</td>\n",
       "      <td>deepset/roberta-base-squad2-distilled</td>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>MultiNLI accuracy</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what is attention?</td>\n",
       "      <td>default</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>multi-head</td>\n",
       "      <td>0.680392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what is attention?</td>\n",
       "      <td>default</td>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>improving many natural language</td>\n",
       "      <td>0.133738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what types of attention are defined?</td>\n",
       "      <td>default</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>multi-head</td>\n",
       "      <td>0.289631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what types of attention are defined?</td>\n",
       "      <td>default</td>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>deep bidirectional representations</td>\n",
       "      <td>0.003118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what is multi-head attention?</td>\n",
       "      <td>default</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>scaled dot-product attention</td>\n",
       "      <td>0.802400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what is multi-head attention?</td>\n",
       "      <td>default</td>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>Language</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               question  \\\n",
       "0                    what is attention?   \n",
       "1                    what is attention?   \n",
       "0  what types of attention are defined?   \n",
       "1  what types of attention are defined?   \n",
       "0         what is multi-head attention?   \n",
       "1         what is multi-head attention?   \n",
       "0                    what is attention?   \n",
       "1                    what is attention?   \n",
       "0  what types of attention are defined?   \n",
       "1  what types of attention are defined?   \n",
       "0         what is multi-head attention?   \n",
       "1         what is multi-head attention?   \n",
       "\n",
       "                              model name  \\\n",
       "0  deepset/roberta-base-squad2-distilled   \n",
       "1  deepset/roberta-base-squad2-distilled   \n",
       "0  deepset/roberta-base-squad2-distilled   \n",
       "1  deepset/roberta-base-squad2-distilled   \n",
       "0  deepset/roberta-base-squad2-distilled   \n",
       "1  deepset/roberta-base-squad2-distilled   \n",
       "0                                default   \n",
       "1                                default   \n",
       "0                                default   \n",
       "1                                default   \n",
       "0                                default   \n",
       "1                                default   \n",
       "\n",
       "                                                                                              paper  \\\n",
       "0                                                        attention is all you need 1706.03762v7.pdf   \n",
       "1  BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "0                                                        attention is all you need 1706.03762v7.pdf   \n",
       "1  BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "0                                                        attention is all you need 1706.03762v7.pdf   \n",
       "1  BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "0                                                        attention is all you need 1706.03762v7.pdf   \n",
       "1  BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "0                                                        attention is all you need 1706.03762v7.pdf   \n",
       "1  BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "0                                                        attention is all you need 1706.03762v7.pdf   \n",
       "1  BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "\n",
       "                                         answer     score  \n",
       "0                                  All You Need  0.038977  \n",
       "1  conceptually simple and empirically powerful  0.000194  \n",
       "0                          multi-head attention  0.000005  \n",
       "1                         self-attention layers  0.000024  \n",
       "0        parameter-free position representation  0.003145  \n",
       "1                             MultiNLI accuracy  0.000003  \n",
       "0                                    multi-head  0.680392  \n",
       "1               improving many natural language  0.133738  \n",
       "0                                    multi-head  0.289631  \n",
       "1            deep bidirectional representations  0.003118  \n",
       "0                  scaled dot-product attention  0.802400  \n",
       "1                                      Language  0.000012  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "35f41f47-f34b-477b-aeff-b9e6f63bd78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['paper', 'answer', 'score']\n"
     ]
    }
   ],
   "source": [
    "merged_df = merged_df.reset_index()\n",
    "cols = ['paper', 'answer', 'score']\n",
    "print(cols)\n",
    "#cols.remove('index')\n",
    "merged_df = merged_df.groupby(['question','model name'])[cols].apply(lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b57d17ab-01eb-454e-b8a4-a197521297b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>paper</th>\n",
       "      <th>answer</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question</th>\n",
       "      <th>model name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">what is attention?</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">deepset/roberta-base-squad2-distilled</th>\n",
       "      <th>0</th>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>All You Need</td>\n",
       "      <td>0.038977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>conceptually simple and empirically powerful</td>\n",
       "      <td>0.000194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">default</th>\n",
       "      <th>6</th>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>multi-head</td>\n",
       "      <td>0.680392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>improving many natural language</td>\n",
       "      <td>0.133738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">what is multi-head attention?</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">deepset/roberta-base-squad2-distilled</th>\n",
       "      <th>4</th>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>parameter-free position representation</td>\n",
       "      <td>0.003145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>MultiNLI accuracy</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">default</th>\n",
       "      <th>10</th>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>scaled dot-product attention</td>\n",
       "      <td>0.802400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>Language</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">what types of attention are defined?</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">deepset/roberta-base-squad2-distilled</th>\n",
       "      <th>2</th>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>multi-head attention</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>self-attention layers</td>\n",
       "      <td>0.000024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">default</th>\n",
       "      <th>8</th>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>multi-head</td>\n",
       "      <td>0.289631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>deep bidirectional representations</td>\n",
       "      <td>0.003118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                          paper  \\\n",
       "question                             model name                                                                                                                                   \n",
       "what is attention?                   deepset/roberta-base-squad2-distilled 0                                                         attention is all you need 1706.03762v7.pdf   \n",
       "                                                                           1   BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "                                     default                               6                                                         attention is all you need 1706.03762v7.pdf   \n",
       "                                                                           7   BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "what is multi-head attention?        deepset/roberta-base-squad2-distilled 4                                                         attention is all you need 1706.03762v7.pdf   \n",
       "                                                                           5   BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "                                     default                               10                                                        attention is all you need 1706.03762v7.pdf   \n",
       "                                                                           11  BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "what types of attention are defined? deepset/roberta-base-squad2-distilled 2                                                         attention is all you need 1706.03762v7.pdf   \n",
       "                                                                           3   BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "                                     default                               8                                                         attention is all you need 1706.03762v7.pdf   \n",
       "                                                                           9   BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "\n",
       "                                                                                                                     answer  \\\n",
       "question                             model name                                                                               \n",
       "what is attention?                   deepset/roberta-base-squad2-distilled 0                                   All You Need   \n",
       "                                                                           1   conceptually simple and empirically powerful   \n",
       "                                     default                               6                                     multi-head   \n",
       "                                                                           7                improving many natural language   \n",
       "what is multi-head attention?        deepset/roberta-base-squad2-distilled 4         parameter-free position representation   \n",
       "                                                                           5                              MultiNLI accuracy   \n",
       "                                     default                               10                  scaled dot-product attention   \n",
       "                                                                           11                                      Language   \n",
       "what types of attention are defined? deepset/roberta-base-squad2-distilled 2                           multi-head attention   \n",
       "                                                                           3                          self-attention layers   \n",
       "                                     default                               8                                     multi-head   \n",
       "                                                                           9             deep bidirectional representations   \n",
       "\n",
       "                                                                                  score  \n",
       "question                             model name                                          \n",
       "what is attention?                   deepset/roberta-base-squad2-distilled 0   0.038977  \n",
       "                                                                           1   0.000194  \n",
       "                                     default                               6   0.680392  \n",
       "                                                                           7   0.133738  \n",
       "what is multi-head attention?        deepset/roberta-base-squad2-distilled 4   0.003145  \n",
       "                                                                           5   0.000003  \n",
       "                                     default                               10  0.802400  \n",
       "                                                                           11  0.000012  \n",
       "what types of attention are defined? deepset/roberta-base-squad2-distilled 2   0.000005  \n",
       "                                                                           3   0.000024  \n",
       "                                     default                               8   0.289631  \n",
       "                                                                           9   0.003118  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64f8ce2-30dc-4e97-a724-b2325e3655a9",
   "metadata": {},
   "source": [
    "## 3. Sematic Vector Search: Hands-On "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1f486ce6-6467-4cd6-a5ac-01d4ca7226a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e4632c2d-657a-4aff-8f57-4291833fc99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'all-mpnet-base-v2'\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc47426-f755-4af4-9869-b5e5e725dd12",
   "metadata": {},
   "source": [
    "<b>3.1 Semantic Search Accross Text Chunks</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5797c17e-fa1e-4a51-81ff-529bdbbc6107",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = ['what is attention?', 'what is multi-head attention?']\n",
    "documents = [] \n",
    "for rec in all_docs:\n",
    "    text_chunk = rec.text.replace('.\\n','. ')\n",
    "    text_chunk = text_chunk.replace('\\n',' ')\n",
    "    documents.append(text_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b4452217-ac21-4bbc-ae0a-3e0fa2271812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works. Attention Is All You Need Ashish Vaswani∗ Google Brain avaswani@google.com Noam Shazeer∗ Google Brain noam@google.com Niki Parmar∗ Google Research nikip@google.com Jakob Uszkoreit∗ Google Research usz@google.com Llion Jones∗ Google Research llion@google.com Aidan N. Gomez∗ † University of Toronto aidan@cs.toronto.edu Łukasz Kaiser∗ Google Brain lukaszkaiser@google.com Illia Polosukhin∗ ‡ illia.polosukhin@gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. ∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research. †Work performed while at Google Brain. ‡Work performed while at Google Research. 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA. arXiv:1706.03762v7  [cs.CL]  2 Aug 2023'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bde66917-acef-4938-ad6e-3b0b52a25c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embeddings = model.encode(queries) \n",
    "document_embeddings = model.encode(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6bf95a78-060b-4ab7-930c-2c1eb3d24540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 768)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d929b538-60d7-4d15-b868-06f007e17875",
   "metadata": {},
   "outputs": [],
   "source": [
    "#query_embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "27d84b9e-95c4-41a4-a6de-d6de139332c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 768)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32577555-3b15-42c0-a772-9870cb96bd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#document_embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "372c57d4-dd9e-4ed4-8506-92fafa2072f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.similarity(query_embeddings, document_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1581f92f-de10-4d3c-98ef-5b2d140cb725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.27904153, 0.2639539 , 0.33054262, 0.34323582, 0.36463457,\n",
       "        0.2585139 , 0.2755901 , 0.12498504, 0.32768473, 0.1439856 ,\n",
       "        0.16710263, 0.2609303 , 0.4622292 , 0.40838757, 0.31974906,\n",
       "        0.20027614, 0.16921651, 0.17691593, 0.18058467, 0.22321635,\n",
       "        0.15124284, 0.11138519, 0.17050765, 0.19434658, 0.1487274 ,\n",
       "        0.14039241, 0.2860044 , 0.20682164, 0.19277057, 0.1614857 ,\n",
       "        0.10827111],\n",
       "       [0.33957446, 0.24378414, 0.4095444 , 0.37700048, 0.45007756,\n",
       "        0.23550165, 0.28207815, 0.2192395 , 0.37530297, 0.18709067,\n",
       "        0.18889803, 0.2374867 , 0.5049366 , 0.43961325, 0.44466537,\n",
       "        0.20438865, 0.19514653, 0.2123058 , 0.16955382, 0.20691344,\n",
       "        0.22477685, 0.21884929, 0.25725156, 0.18618935, 0.18130736,\n",
       "        0.17228976, 0.35454687, 0.28306875, 0.18855003, 0.17762521,\n",
       "        0.1183356 ]], dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d53a28ba-f5af-48f7-a6ce-12a3d56dc733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: what is attention?\n",
      "0.4622292 Attention Visualizations Input-Input Layer5 It is in this spirit that a majority of American governm\n",
      "0.40838757 Input-Input Layer5 The Law will never be perfect , but its application should be just - this is what\n",
      "0.36463457 output values. These are concatenated and once again projected, resulting in the final values, as de\n",
      "0.34323582 Scaled Dot-Product Attention  Multi-Head Attention Figure 2: (left) Scaled Dot-Product Attention. (r\n",
      "0.33054262 Figure 1: The Transformer - model architecture. The Transformer follows this overall architecture us\n",
      "0.32768473 Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the b\n",
      "0.31974906 Input-Input Layer5 The Law will never be perfect , but its application should be just - this is what\n",
      "0.2860044 for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: An- alyzi\n",
      "0.27904153 Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and\n",
      "0.2755901 length n is smaller than the representation dimensionality d, which is most often the case with sent\n",
      "0.2639539 1 Introduction Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural\n",
      "0.2609303 [25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated c\n",
      "0.2585139 Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for \n",
      "Query: what is multi-head attention?\n",
      "0.5049366 Attention Visualizations Input-Input Layer5 It is in this spirit that a majority of American governm\n",
      "0.45007756 output values. These are concatenated and once again projected, resulting in the final values, as de\n",
      "0.44466537 Input-Input Layer5 The Law will never be perfect , but its application should be just - this is what\n",
      "0.43961325 Input-Input Layer5 The Law will never be perfect , but its application should be just - this is what\n",
      "0.4095444 Figure 1: The Transformer - model architecture. The Transformer follows this overall architecture us\n",
      "0.37700048 Scaled Dot-Product Attention  Multi-Head Attention Figure 2: (left) Scaled Dot-Product Attention. (r\n",
      "0.37530297 Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the b\n",
      "0.35454687 for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: An- alyzi\n",
      "0.33957446 Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and\n",
      "0.28306875 BERT (Ours) Trm Trm Trm Trm Trm Trm ... ... Trm Trm Trm Trm Trm Trm ... ... OpenAI GPT Lstm ELMo Lst\n",
      "0.28207815 length n is smaller than the representation dimensionality d, which is most often the case with sent\n",
      "0.25725156 Dev Set Tasks MNLI-m QNLI MRPC SST-2 SQuAD (Acc) (Acc) (Acc) (Acc) (F1) BERTBASE 84.4 88.4 86.7 92.7\n"
     ]
    }
   ],
   "source": [
    "# Output the results\n",
    "thres = 0.25\n",
    "\n",
    "query_score_list = []\n",
    "\n",
    "for query, query_scores in zip(queries, scores):\n",
    "    doc_score_pairs = list(zip(documents, query_scores))\n",
    "    doc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n",
    "    print(\"Query:\", query)\n",
    "    k=-1\n",
    "    for document, score in doc_score_pairs:\n",
    "        k += 1\n",
    "        score_val = score.numpy()\n",
    "        if score_val >= 0.25:\n",
    "            print(score_val, document[:100])\n",
    "            query_score_list.append({'query':query, \n",
    "                                     'doc':document,\n",
    "                                     'llama_index_doc': all_docs[k],\n",
    "                                     'score': score_val\n",
    "                                    })        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c38f4c-b6de-4af5-b9ed-b6eef6f87020",
   "metadata": {},
   "source": [
    "<b>3.2 Semantic Search Inside Text Chunks</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "90646135-9446-4fcb-9069-de97c3dfc240",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 2\n",
    "doc = query_score_list[ind]['doc']\n",
    "q = query_score_list[ind]['query']\n",
    "\n",
    "doc_sentence = doc.split('. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cc193017-d1d3-42f4-a843-fd2cdb3266ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what is attention?'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3ed1a270-7bcc-458c-9979-b67e3f0da71b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['output values',\n",
       " 'These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2',\n",
       " 'Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions',\n",
       " 'With a single attention head, averaging inhibits this',\n",
       " 'MultiHead(Q, K, V) = Concat(head1, ...,headh)WO where headi = Attention(QWQ i , KWK i , V WV i ) Where the projections are parameter matricesWQ i ∈ Rdmodel×dk , WK i ∈ Rdmodel×dk , WV i ∈ Rdmodel×dv and WO ∈ Rhdv×dmodel ',\n",
       " 'In this work we employ h = 8 parallel attention layers, or heads',\n",
       " 'For each of these we use dk = dv = dmodel/h = 64',\n",
       " 'Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality',\n",
       " '3.2.3 Applications of Attention in our Model The Transformer uses multi-head attention in three different ways: • In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder',\n",
       " 'This allows every position in the decoder to attend over all positions in the input sequence',\n",
       " 'This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9]',\n",
       " '• The encoder contains self-attention layers',\n",
       " 'In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder',\n",
       " 'Each position in the encoder can attend to all positions in the previous layer of the encoder',\n",
       " '• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position',\n",
       " 'We need to prevent leftward information flow in the decoder to preserve the auto-regressive property',\n",
       " 'We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections',\n",
       " 'See Figure 2',\n",
       " '3.3 Position-wise Feed-Forward Networks In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically',\n",
       " 'This consists of two linear transformations with a ReLU activation in between',\n",
       " 'FFN(x) = max(0, xW1 + b1)W2 + b2 (2) While the linear transformations are the same across different positions, they use different parameters from layer to layer',\n",
       " 'Another way of describing this is as two convolutions with kernel size 1',\n",
       " 'The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality dff = 2048',\n",
       " '3.4 Embeddings and Softmax Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel',\n",
       " 'We also use the usual learned linear transfor- mation and softmax function to convert the decoder output to predicted next-token probabilities',\n",
       " 'In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [30]',\n",
       " 'In the embedding layers, we multiply those weights by √dmodel',\n",
       " '5']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6d76fc92-58cc-4306-8a84-7803f11cd11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_query_embeddings = model.encode(q) \n",
    "doc_sentence_embeddings = model.encode(doc_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "94c29d14-16a0-41dc-b757-42f18d7aacf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_scores = model.similarity(doc_query_embeddings, \n",
    "                                   doc_sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2ad12f58-2a08-4c09-a9db-5c801da0e493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0960,  0.0322,  0.4273,  0.4587,  0.2416,  0.4568,  0.0357,  0.4336,\n",
       "          0.4957,  0.2036,  0.3647,  0.3706,  0.3717,  0.1514,  0.3282,  0.0843,\n",
       "          0.2573,  0.1392,  0.3269,  0.1512, -0.0143,  0.1179,  0.1034,  0.1406,\n",
       "          0.1311,  0.0234,  0.0249,  0.1270]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "448d1205-c7f3-403f-885f-d5477486c1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "import matplotlib as matplotlib\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "def color_map_color(value, \n",
    "                    cmap_name='PuBu',\n",
    "                    #cmap_name='Wistia', \n",
    "                    vmin=0, \n",
    "                    vmax=1):\n",
    "    # norm = plt.Normalize(vmin, vmax)\n",
    "    norm = matplotlib.colors.Normalize(vmin=vmin, vmax=vmax)\n",
    "    cmap = cm.get_cmap(cmap_name)  # PiYG\n",
    "    rgb = cmap(norm(abs(value)))[:3]  # will return rgba, we take only first 3 so we get rgb\n",
    "    color = matplotlib.colors.rgb2hex(rgb)\n",
    "    return color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "170654e9-32be-46da-8e03-53bf6d15695a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\18623\\AppData\\Local\\Temp\\ipykernel_12208\\4137129256.py:12: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  cmap = cm.get_cmap(cmap_name)  # PiYG\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"<span style='background-color:#6fa7ce'>text</span>\""
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_template = \"<span style='background-color:{}'>{}</span>\"\n",
    "html_template.format(color_map_color(0.51),'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "614db170-8459-4459-b0aa-fcd5bf5f1fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\18623\\AppData\\Local\\Temp\\ipykernel_12208\\4137129256.py:12: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  cmap = cm.get_cmap(cmap_name)  # PiYG\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<span style='background-color:#6fa7ce'>text</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(html_template.format(color_map_color(0.51),'text')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f3035211-c33d-4735-b95b-7fde23b54f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\18623\\AppData\\Local\\Temp\\ipykernel_12208\\4137129256.py:12: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  cmap = cm.get_cmap(cmap_name)  # PiYG\n"
     ]
    }
   ],
   "source": [
    "html_template = \"<span style='background-color:{};opacity:0.8;'>{}</span><sup style='font-size:10px;font-weight:bold;color:red;'>Score:{:.2f}</sup>\"\n",
    "html_output = []\n",
    "for query, query_scores in zip(q, sentence_scores):\n",
    "    doc_score_pairs = list(zip(doc_sentence, query_scores))\n",
    "    doc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n",
    "    #print(\"Query:\", q)\n",
    "    for document, score in doc_score_pairs:\n",
    "        score_val = score.numpy()\n",
    "        html_output.append(html_template.format(color_map_color(1-score_val),document,score_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3c42aeac-4f85-4d88-8752-27f3401b6f77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span style='background-color:#71a8ce;opacity:0.8;'>3.2.3 Applications of Attention in our Model The Transformer uses multi-head attention in three different ways: • In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder</span><sup style='font-size:10px;font-weight:bold;color:red;'>Score:0.50</sup> <span style='background-color:#60a1ca;opacity:0.8;'>With a single attention head, averaging inhibits this</span><sup style='font-size:10px;font-weight:bold;color:red;'>Score:0.46</sup> <span style='background-color:#5ea0ca;opacity:0.8;'>In this work we employ h = 8 parallel attention layers, or heads</span><sup style='font-size:10px;font-weight:bold;color:red;'>Score:0.46</sup> <span style='background-color:#529bc7;opacity:0.8;'>Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality</span><sup style='font-size:10px;font-weight:bold;color:red;'>Score:0.43</sup> <span style='background-color:#509ac6;opacity:0.8;'>Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions</span><sup style='font-size:10px;font-weight:bold;color:red;'>Score:0.43</sup> <span style='background-color:#358fc0;opacity:0.8;'>In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder</span><sup style='font-size:10px;font-weight:bold;color:red;'>Score:0.37</sup> <span style='background-color:#348ebf;opacity:0.8;'>• The encoder contains self-attention layers</span><sup style='font-size:10px;font-weight:bold;color:red;'>Score:0.37</sup> <span style='background-color:#328dbf;opacity:0.8;'>This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9]</span><sup style='font-size:10px;font-weight:bold;color:red;'>Score:0.36</sup> <span style='background-color:#2484ba;opacity:0.8;'>• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position</span><sup style='font-size:10px;font-weight:bold;color:red;'>Score:0.33</sup> <span style='background-color:#2383ba;opacity:0.8;'>3.3 Position-wise Feed-Forward Networks In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically</span><sup style='font-size:10px;font-weight:bold;color:red;'>Score:0.33</sup> <span style='background-color:#0771b1;opacity:0.8;'>We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections</span><sup style='font-size:10px;font-weight:bold;color:red;'>Score:0.26</sup> <span style='background-color:#056ead;opacity:0.8;'>MultiHead(Q, K, V) = Concat(head1, ...,headh)WO where headi = Attention(QWQ i , KWK i , V WV i ) Where the projections are parameter matricesWQ i ∈ Rdmodel×dk , WK i ∈ Rdmodel×dk , WV i ∈ Rdmodel×dv and WO ∈ Rhdv×dmodel </span><sup style='font-size:10px;font-weight:bold;color:red;'>Score:0.24</sup> <span style='background-color:#0568a3;opacity:0.8;'>This allows every position in the decoder to attend over all positions in the input sequence</span><sup style='font-size:10px;font-weight:bold;color:red;'>Score:0.20</sup> <span style='background-color:#045e94;opacity:0.8;'>Each position in the encoder can attend to all positions in the previous layer of the encoder</span><sup style='font-size:10px;font-weight:bold;color:red;'>Score:0.15</sup> <span style='background-color:#045e94;opacity:0.8;'>This consists of two linear transformations with a ReLU activation in between</span><sup style='font-size:10px;font-weight:bold;color:red;'>Score:0.15</sup> <span style='background-color:#045c90;opacity:0.8;'>3.4 Embeddings and Softmax Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel</span><sup style='font-size:10px;font-weight:bold;color:red;'>Score:0.14</sup> <span style='background-color:#045c90;opacity:0.8;'>See Figure 2</span><sup style='font-size:10px;font-weight:bold;color:red;'>Score:0.14</sup> <span style='background-color:#045b8e;opacity:0.8;'>We also use the usual learned linear transfor- mation and softmax function to convert the decoder output to predicted next-token probabilities</span><sup style='font-size:10px;font-weight:bold;color:red;'>Score:0.13</sup> <span style='background-color:#045a8d;opacity:0.8;'>5</span><sup style='font-size:10px;font-weight:bold;color:red;'>Score:0.13</sup> <span style='background-color:#04588a;opacity:0.8;'>Another way of describing this is as two convolutions with kernel size 1</span><sup style='font-size:10px;font-weight:bold;color:red;'>Score:0.12</sup> <span style='background-color:#045483;opacity:0.8;'>The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality dff = 2048</span><sup style='font-size:10px;font-weight:bold;color:red;'>Score:0.10</sup> <span style='background-color:#045280;opacity:0.8;'>output values</span><sup style='font-size:10px;font-weight:bold;color:red;'>Score:0.10</sup> <span style='background-color:#034e7b;opacity:0.8;'>We need to prevent leftward information flow in the decoder to preserve the auto-regressive property</span><sup style='font-size:10px;font-weight:bold;color:red;'>Score:0.08</sup> <span style='background-color:#034267;opacity:0.8;'>For each of these we use dk = dv = dmodel/h = 64</span><sup style='font-size:10px;font-weight:bold;color:red;'>Score:0.04</sup> <span style='background-color:#034165;opacity:0.8;'>These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2</span><sup style='font-size:10px;font-weight:bold;color:red;'>Score:0.03</sup> <span style='background-color:#023e62;opacity:0.8;'>In the embedding layers, we multiply those weights by √dmodel</span><sup style='font-size:10px;font-weight:bold;color:red;'>Score:0.02</sup> <span style='background-color:#023d60;opacity:0.8;'>In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [30]</span><sup style='font-size:10px;font-weight:bold;color:red;'>Score:0.02</sup> <span style='background-color:#023858;opacity:0.8;'>FFN(x) = max(0, xW1 + b1)W2 + b2 (2) While the linear transformations are the same across different positions, they use different parameters from layer to layer</span><sup style='font-size:10px;font-weight:bold;color:red;'>Score:-0.01</sup>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(' '.join(html_output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758c55fc-356d-4f6a-b183-e4d52233052b",
   "metadata": {},
   "source": [
    "## 4. Semantic Vector Search: Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "58bf1575-0e70-4e69-87cf-90f9aec2997e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "98ddb757-ef90-4b48-b292-6f8864b79471",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "624445b2-348a-4d59-ba8f-746465c16550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store docs into vector DB\n",
    "index = VectorStoreIndex.from_documents(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0bf6540c-2503-4258-bdc1-c09e3abbe821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='757dfa3d-eac9-419b-b818-8d82813df6c6', embedding=None, metadata={'page_label': '1', 'file_name': 'attention is all you need 1706.03762v7.pdf', 'file_path': 'C:\\\\Users\\\\18623\\\\Desktop\\\\PhiAi\\\\Jupyter\\\\papers\\\\attention is all you need 1706.03762v7.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-12-11', 'last_modified_date': '2024-12-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "35f22dec-7949-42ef-9b09-ca5c5274918f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of docs to retreive\n",
    "top_k = 7\n",
    "\n",
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=top_k,\n",
    ")\n",
    "     \n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.3)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76b6c14-fc5c-4268-86ed-20d486edcfc7",
   "metadata": {},
   "source": [
    "<b>4.1 To BERT or not to BERT? this is the questions</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "91e045ed-8627-47e5-886b-3d812f05c895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query documents\n",
    "query = \"What is BERT?\"\n",
    "response = query_engine.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7c4c489a-81c7-4903-8b73-3a47f50cc171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['response', 'source_nodes', 'metadata'])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "25cd522b-6362-45d7-bafe-3520aab862a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.__dict__['source_nodes'][0].node.__dict__['metadata']['file_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0d3e2543-eb6a-4de2-b112-a4e6b5dcb847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat response\n",
    "cols = ['question','scores','paper','text']\n",
    "df_dict = {col:[] for col in cols}\n",
    "\n",
    "for i in range(top_k):\n",
    "    #context += f'>>> chunk {i+1} ' + str(response.source_nodes[i].score) + '\\n\\n' + response.source_nodes[i].text + \"\\n\\n\"\n",
    "    #context += 'From: '+response.__dict__['source_nodes'][i].node.__dict__['metadata']['file_name'] + '\\n\\n'\n",
    "    df_dict['scores'].append(response.source_nodes[i].score)\n",
    "    df_dict['paper'].append(response.__dict__['source_nodes'][i].node.__dict__['metadata']['file_name'])\n",
    "    df_dict['text'].append(response.source_nodes[i].text)\n",
    "\n",
    "df_dict['question'] = [query]*len(df_dict['scores'])\n",
    "output_df[query] = pd.DataFrame(df_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9a88cfaa-2206-4ddc-b661-03e12416a13a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>scores</th>\n",
       "      <th>paper</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is BERT?</td>\n",
       "      <td>0.811123</td>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>BERT is the ﬁrst ﬁne-\\ntuning based representation model that achieves\\nstate-of-the-art performance on a large suite\\nof sentence-level and token-level tasks, outper-\\nforming many task-speciﬁc architectures.\\n• BERT advances the state of the art for eleven\\nNLP tasks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is BERT?</td>\n",
       "      <td>0.751915</td>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>BERT is conceptually simple and empirically\\npowerful. It obtains new state-of-the-art re-\\nsults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is BERT?</td>\n",
       "      <td>0.751781</td>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>BERT BERT\\nE[CLS] E1  E[SEP]. EN E1’ . EM’\\nC\\n T1\\n T[SEP].\\n TN\\n T1’ .\\n TM’\\n[CLS] Tok 1  [SEP]. Tok N Tok 1 . TokM\\nQuestion Paragraph\\nStart/End Span\\nBERT\\nE[CLS] E1  E[SEP].</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is BERT?</td>\n",
       "      <td>0.740903</td>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>TokM\\nQuestion Paragraph\\nStart/End Span\\nBERT\\nE[CLS] E1  E[SEP]. EN E1’ . EM’\\nC\\n T1\\n T[SEP].\\n TN\\n T1’ .\\n TM’\\n[CLS] Tok 1  [SEP]. Tok N Tok 1 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is BERT?</td>\n",
       "      <td>0.738021</td>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>Input/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g., ⟨Question, Answer ⟩) in one token sequence.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What is BERT?</td>\n",
       "      <td>0.733836</td>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>Each downstream task has sep-\\narate ﬁne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\nA distinctive feature of BERT is its uniﬁed ar-\\nchitecture across different tasks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What is BERT?</td>\n",
       "      <td>0.731738</td>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>BERT\\nE[CLS] E1  E[SEP]. EN E1’ . EM’\\nC\\n T1\\n T[SEP].\\n TN\\n T1’ .\\n TM’\\n[CLS] Tok \\n1\\n [SEP]. Tok \\nN\\nTok \\n1 .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        question    scores  \\\n",
       "0  What is BERT?  0.811123   \n",
       "1  What is BERT?  0.751915   \n",
       "2  What is BERT?  0.751781   \n",
       "3  What is BERT?  0.740903   \n",
       "4  What is BERT?  0.738021   \n",
       "5  What is BERT?  0.733836   \n",
       "6  What is BERT?  0.731738   \n",
       "\n",
       "                                                                                              paper  \\\n",
       "0  BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "1  BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "2  BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "3  BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "4  BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "5  BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "6  BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                   text  \n",
       "0                                        BERT is the ﬁrst ﬁne-\\ntuning based representation model that achieves\\nstate-of-the-art performance on a large suite\\nof sentence-level and token-level tasks, outper-\\nforming many task-speciﬁc architectures.\\n• BERT advances the state of the art for eleven\\nNLP tasks.  \n",
       "1                     BERT is conceptually simple and empirically\\npowerful. It obtains new state-of-the-art re-\\nsults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.  \n",
       "2                                                                                                                                 BERT BERT\\nE[CLS] E1  E[SEP]. EN E1’ . EM’\\nC\\n T1\\n T[SEP].\\n TN\\n T1’ .\\n TM’\\n[CLS] Tok 1  [SEP]. Tok N Tok 1 . TokM\\nQuestion Paragraph\\nStart/End Span\\nBERT\\nE[CLS] E1  E[SEP].  \n",
       "3                                                                                                                                                               TokM\\nQuestion Paragraph\\nStart/End Span\\nBERT\\nE[CLS] E1  E[SEP]. EN E1’ . EM’\\nC\\n T1\\n T[SEP].\\n TN\\n T1’ .\\n TM’\\n[CLS] Tok 1  [SEP]. Tok N Tok 1 .  \n",
       "4                                                                    Input/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g., ⟨Question, Answer ⟩) in one token sequence.  \n",
       "5  Each downstream task has sep-\\narate ﬁne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\nA distinctive feature of BERT is its uniﬁed ar-\\nchitecture across different tasks.  \n",
       "6                                                                                                                                                                                                 BERT\\nE[CLS] E1  E[SEP]. EN E1’ . EM’\\nC\\n T1\\n T[SEP].\\n TN\\n T1’ .\\n TM’\\n[CLS] Tok \\n1\\n [SEP]. Tok \\nN\\nTok \\n1 .  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df[query]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1566ddbb-1628-45cc-ba2a-1b3befd843f5",
   "metadata": {},
   "source": [
    "<b>4.2 Pay Attention</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "329672a0-323a-4aa9-a804-e0bb8c7b8474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query documents\n",
    "query = \"What is multi-head attention?\"\n",
    "response = query_engine.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "cf687120-e8e8-4939-9e0c-ae076cfba69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat response\n",
    "cols = ['question','scores','paper','text']\n",
    "df_dict = {col:[] for col in cols}\n",
    "\n",
    "for i in range(top_k):\n",
    "    #context += f'>>> chunk {i+1} ' + str(response.source_nodes[i].score) + '\\n\\n' + response.source_nodes[i].text + \"\\n\\n\"\n",
    "    #context += 'From: '+response.__dict__['source_nodes'][i].node.__dict__['metadata']['file_name'] + '\\n\\n'\n",
    "    df_dict['scores'].append(response.source_nodes[i].score)\n",
    "    df_dict['paper'].append(response.__dict__['source_nodes'][i].node.__dict__['metadata']['file_name'])\n",
    "    df_dict['text'].append(response.source_nodes[i].text)\n",
    "\n",
    "df_dict['question'] = [query]*len(df_dict['scores'])\n",
    "output_df[query] = pd.DataFrame(df_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "531498e4-f0b4-43d5-afb5-a54bc1f1bc2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>scores</th>\n",
       "      <th>paper</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is multi-head attention?</td>\n",
       "      <td>0.800710</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is multi-head attention?</td>\n",
       "      <td>0.784158</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is multi-head attention?</td>\n",
       "      <td>0.772050</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is multi-head attention?</td>\n",
       "      <td>0.758912</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\nwhere headi = Attention(QWQ\\ni , KWK\\ni , V WV\\ni )\\nWhere the projections are parameter matricesWQ\\ni ∈ Rdmodel×dk , WK\\ni ∈ Rdmodel×dk , WV\\ni ∈ Rdmodel×dv\\nand WO ∈ Rhdv×dmodel .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is multi-head attention?</td>\n",
       "      <td>0.740496</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [ 1].</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What is multi-head attention?</td>\n",
       "      <td>0.738943</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>To counteract this effect, we scale the dot products by 1√dk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What is multi-head attention?</td>\n",
       "      <td>0.738749</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>In this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        question    scores  \\\n",
       "0  What is multi-head attention?  0.800710   \n",
       "1  What is multi-head attention?  0.784158   \n",
       "2  What is multi-head attention?  0.772050   \n",
       "3  What is multi-head attention?  0.758912   \n",
       "4  What is multi-head attention?  0.740496   \n",
       "5  What is multi-head attention?  0.738943   \n",
       "6  What is multi-head attention?  0.738749   \n",
       "\n",
       "                                        paper  \\\n",
       "0  attention is all you need 1706.03762v7.pdf   \n",
       "1  attention is all you need 1706.03762v7.pdf   \n",
       "2  attention is all you need 1706.03762v7.pdf   \n",
       "3  attention is all you need 1706.03762v7.pdf   \n",
       "4  attention is all you need 1706.03762v7.pdf   \n",
       "5  attention is all you need 1706.03762v7.pdf   \n",
       "6  attention is all you need 1706.03762v7.pdf   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                               text  \n",
       "0                                             Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.  \n",
       "1  3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence.  \n",
       "2                                                          output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.  \n",
       "3                                                                                    With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\nwhere headi = Attention(QWQ\\ni , KWK\\ni , V WV\\ni )\\nWhere the projections are parameter matricesWQ\\ni ∈ Rdmodel×dk , WK\\ni ∈ Rdmodel×dk , WV\\ni ∈ Rdmodel×dv\\nand WO ∈ Rhdv×dmodel .  \n",
       "4                                                                                              Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [ 1].  \n",
       "5          To counteract this effect, we scale the dot products by 1√dk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively.  \n",
       "6                                                                                                               In this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df[query]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bce1b1-2a1a-4f39-89fe-cbf6d3156da0",
   "metadata": {},
   "source": [
    "<b>4.3 Who let Transformer out?</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2263b187-6cda-417f-ad26-49a7fa71fa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query documents\n",
    "query = \"What is Transformer?\"\n",
    "response = query_engine.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "45782111-c557-4941-8a31-fd2a4590d5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat response\n",
    "cols = ['question','scores','paper','text']\n",
    "df_dict = {col:[] for col in cols}\n",
    "\n",
    "for i in range(top_k):\n",
    "    #context += f'>>> chunk {i+1} ' + str(response.source_nodes[i].score) + '\\n\\n' + response.source_nodes[i].text + \"\\n\\n\"\n",
    "    #context += 'From: '+response.__dict__['source_nodes'][i].node.__dict__['metadata']['file_name'] + '\\n\\n'\n",
    "    df_dict['scores'].append(response.source_nodes[i].score)\n",
    "    df_dict['paper'].append(response.__dict__['source_nodes'][i].node.__dict__['metadata']['file_name'])\n",
    "    df_dict['text'].append(response.source_nodes[i].text)\n",
    "\n",
    "df_dict['question'] = [query]*len(df_dict['scores'])\n",
    "output_df[query] = pd.DataFrame(df_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7783af40-4677-4123-9f18-d7026e9fe697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>scores</th>\n",
       "      <th>paper</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is Transformer?</td>\n",
       "      <td>0.696613</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is Transformer?</td>\n",
       "      <td>0.695771</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>The Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is Transformer?</td>\n",
       "      <td>0.679655</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is Transformer?</td>\n",
       "      <td>0.674317</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is Transformer?</td>\n",
       "      <td>0.668299</td>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>For example, the largest Transformer explored in\\nVaswani et al. (2017) is (L=6, H=1024, A=16)\\nwith 100M parameters for the encoder, and the\\nlargest Transformer we have found in the literature\\nis (L=64, H=512, A=2) with 235M parameters\\n(Al-Rfou et al., 2018).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What is Transformer?</td>\n",
       "      <td>0.662332</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What is Transformer?</td>\n",
       "      <td>0.657973</td>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>former is often referred to as a “Transformer encoder” while\\nthe left-context-only version is referred to as a “Transformer\\ndecoder” since it can be used for text generation.\\nIn order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               question    scores  \\\n",
       "0  What is Transformer?  0.696613   \n",
       "1  What is Transformer?  0.695771   \n",
       "2  What is Transformer?  0.679655   \n",
       "3  What is Transformer?  0.674317   \n",
       "4  What is Transformer?  0.668299   \n",
       "5  What is Transformer?  0.662332   \n",
       "6  What is Transformer?  0.657973   \n",
       "\n",
       "                                                                                              paper  \\\n",
       "0                                                        attention is all you need 1706.03762v7.pdf   \n",
       "1                                                        attention is all you need 1706.03762v7.pdf   \n",
       "2                                                        attention is all you need 1706.03762v7.pdf   \n",
       "3                                                        attention is all you need 1706.03762v7.pdf   \n",
       "4  BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "5                                                        attention is all you need 1706.03762v7.pdf   \n",
       "6  BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                 text  \n",
       "0                                                                                                                                                 6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8  \n",
       "1                                                                                                                                                                                                                                                                    The Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.  \n",
       "2                                                    Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers.  \n",
       "3  End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution.  \n",
       "4                                                                                                                                                                                             For example, the largest Transformer explored in\\nVaswani et al. (2017) is (L=6, H=1024, A=16)\\nwith 100M parameters for the encoder, and the\\nlargest Transformer we have found in the literature\\nis (L=64, H=512, A=2) with 235M parameters\\n(Al-Rfou et al., 2018).  \n",
       "5                                                                     7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers.  \n",
       "6                                                                                                                   former is often referred to as a “Transformer encoder” while\\nthe left-context-only version is referred to as a “Transformer\\ndecoder” since it can be used for text generation.\\nIn order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens.  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df[query]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "dda91111-0a42-47a6-afcb-966564fa81aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>scores</th>\n",
       "      <th>paper</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is BERT?</td>\n",
       "      <td>0.811123</td>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>BERT is the ﬁrst ﬁne-\\ntuning based representation model that achieves\\nstate-of-the-art performance on a large suite\\nof sentence-level and token-level tasks, outper-\\nforming many task-speciﬁc architectures.\\n• BERT advances the state of the art for eleven\\nNLP tasks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is BERT?</td>\n",
       "      <td>0.751915</td>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>BERT is conceptually simple and empirically\\npowerful. It obtains new state-of-the-art re-\\nsults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is BERT?</td>\n",
       "      <td>0.751781</td>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>BERT BERT\\nE[CLS] E1  E[SEP]. EN E1’ . EM’\\nC\\n T1\\n T[SEP].\\n TN\\n T1’ .\\n TM’\\n[CLS] Tok 1  [SEP]. Tok N Tok 1 . TokM\\nQuestion Paragraph\\nStart/End Span\\nBERT\\nE[CLS] E1  E[SEP].</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is BERT?</td>\n",
       "      <td>0.740903</td>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>TokM\\nQuestion Paragraph\\nStart/End Span\\nBERT\\nE[CLS] E1  E[SEP]. EN E1’ . EM’\\nC\\n T1\\n T[SEP].\\n TN\\n T1’ .\\n TM’\\n[CLS] Tok 1  [SEP]. Tok N Tok 1 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is BERT?</td>\n",
       "      <td>0.738021</td>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>Input/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g., ⟨Question, Answer ⟩) in one token sequence.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What is BERT?</td>\n",
       "      <td>0.733836</td>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>Each downstream task has sep-\\narate ﬁne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\nA distinctive feature of BERT is its uniﬁed ar-\\nchitecture across different tasks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What is BERT?</td>\n",
       "      <td>0.731738</td>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>BERT\\nE[CLS] E1  E[SEP]. EN E1’ . EM’\\nC\\n T1\\n T[SEP].\\n TN\\n T1’ .\\n TM’\\n[CLS] Tok \\n1\\n [SEP]. Tok \\nN\\nTok \\n1 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is multi-head attention?</td>\n",
       "      <td>0.800710</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is multi-head attention?</td>\n",
       "      <td>0.784158</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is multi-head attention?</td>\n",
       "      <td>0.772050</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is multi-head attention?</td>\n",
       "      <td>0.758912</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\nwhere headi = Attention(QWQ\\ni , KWK\\ni , V WV\\ni )\\nWhere the projections are parameter matricesWQ\\ni ∈ Rdmodel×dk , WK\\ni ∈ Rdmodel×dk , WV\\ni ∈ Rdmodel×dv\\nand WO ∈ Rhdv×dmodel .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is multi-head attention?</td>\n",
       "      <td>0.740496</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [ 1].</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What is multi-head attention?</td>\n",
       "      <td>0.738943</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>To counteract this effect, we scale the dot products by 1√dk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What is multi-head attention?</td>\n",
       "      <td>0.738749</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>In this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is Transformer?</td>\n",
       "      <td>0.696613</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is Transformer?</td>\n",
       "      <td>0.695771</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>The Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is Transformer?</td>\n",
       "      <td>0.679655</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is Transformer?</td>\n",
       "      <td>0.674317</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is Transformer?</td>\n",
       "      <td>0.668299</td>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>For example, the largest Transformer explored in\\nVaswani et al. (2017) is (L=6, H=1024, A=16)\\nwith 100M parameters for the encoder, and the\\nlargest Transformer we have found in the literature\\nis (L=64, H=512, A=2) with 235M parameters\\n(Al-Rfou et al., 2018).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What is Transformer?</td>\n",
       "      <td>0.662332</td>\n",
       "      <td>attention is all you need 1706.03762v7.pdf</td>\n",
       "      <td>7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What is Transformer?</td>\n",
       "      <td>0.657973</td>\n",
       "      <td>BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</td>\n",
       "      <td>former is often referred to as a “Transformer encoder” while\\nthe left-context-only version is referred to as a “Transformer\\ndecoder” since it can be used for text generation.\\nIn order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        question    scores  \\\n",
       "0                  What is BERT?  0.811123   \n",
       "1                  What is BERT?  0.751915   \n",
       "2                  What is BERT?  0.751781   \n",
       "3                  What is BERT?  0.740903   \n",
       "4                  What is BERT?  0.738021   \n",
       "5                  What is BERT?  0.733836   \n",
       "6                  What is BERT?  0.731738   \n",
       "0  What is multi-head attention?  0.800710   \n",
       "1  What is multi-head attention?  0.784158   \n",
       "2  What is multi-head attention?  0.772050   \n",
       "3  What is multi-head attention?  0.758912   \n",
       "4  What is multi-head attention?  0.740496   \n",
       "5  What is multi-head attention?  0.738943   \n",
       "6  What is multi-head attention?  0.738749   \n",
       "0           What is Transformer?  0.696613   \n",
       "1           What is Transformer?  0.695771   \n",
       "2           What is Transformer?  0.679655   \n",
       "3           What is Transformer?  0.674317   \n",
       "4           What is Transformer?  0.668299   \n",
       "5           What is Transformer?  0.662332   \n",
       "6           What is Transformer?  0.657973   \n",
       "\n",
       "                                                                                              paper  \\\n",
       "0  BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "1  BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "2  BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "3  BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "4  BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "5  BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "6  BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "0                                                        attention is all you need 1706.03762v7.pdf   \n",
       "1                                                        attention is all you need 1706.03762v7.pdf   \n",
       "2                                                        attention is all you need 1706.03762v7.pdf   \n",
       "3                                                        attention is all you need 1706.03762v7.pdf   \n",
       "4                                                        attention is all you need 1706.03762v7.pdf   \n",
       "5                                                        attention is all you need 1706.03762v7.pdf   \n",
       "6                                                        attention is all you need 1706.03762v7.pdf   \n",
       "0                                                        attention is all you need 1706.03762v7.pdf   \n",
       "1                                                        attention is all you need 1706.03762v7.pdf   \n",
       "2                                                        attention is all you need 1706.03762v7.pdf   \n",
       "3                                                        attention is all you need 1706.03762v7.pdf   \n",
       "4  BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "5                                                        attention is all you need 1706.03762v7.pdf   \n",
       "6  BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                 text  \n",
       "0                                                                                                                                                                                      BERT is the ﬁrst ﬁne-\\ntuning based representation model that achieves\\nstate-of-the-art performance on a large suite\\nof sentence-level and token-level tasks, outper-\\nforming many task-speciﬁc architectures.\\n• BERT advances the state of the art for eleven\\nNLP tasks.  \n",
       "1                                                                                                                                                                   BERT is conceptually simple and empirically\\npowerful. It obtains new state-of-the-art re-\\nsults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.  \n",
       "2                                                                                                                                                                                                                                                                               BERT BERT\\nE[CLS] E1  E[SEP]. EN E1’ . EM’\\nC\\n T1\\n T[SEP].\\n TN\\n T1’ .\\n TM’\\n[CLS] Tok 1  [SEP]. Tok N Tok 1 . TokM\\nQuestion Paragraph\\nStart/End Span\\nBERT\\nE[CLS] E1  E[SEP].  \n",
       "3                                                                                                                                                                                                                                                                                                             TokM\\nQuestion Paragraph\\nStart/End Span\\nBERT\\nE[CLS] E1  E[SEP]. EN E1’ . EM’\\nC\\n T1\\n T[SEP].\\n TN\\n T1’ .\\n TM’\\n[CLS] Tok 1  [SEP]. Tok N Tok 1 .  \n",
       "4                                                                                                                                                                                                                  Input/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g., ⟨Question, Answer ⟩) in one token sequence.  \n",
       "5                                                                                                                                                Each downstream task has sep-\\narate ﬁne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\nA distinctive feature of BERT is its uniﬁed ar-\\nchitecture across different tasks.  \n",
       "6                                                                                                                                                                                                                                                                                                                                               BERT\\nE[CLS] E1  E[SEP]. EN E1’ . EM’\\nC\\n T1\\n T[SEP].\\n TN\\n T1’ .\\n TM’\\n[CLS] Tok \\n1\\n [SEP]. Tok \\nN\\nTok \\n1 .  \n",
       "0                                                                                                                               Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.  \n",
       "1                                                                                    3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence.  \n",
       "2                                                                                                                                            output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.  \n",
       "3                                                                                                                                                                      With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\nwhere headi = Attention(QWQ\\ni , KWK\\ni , V WV\\ni )\\nWhere the projections are parameter matricesWQ\\ni ∈ Rdmodel×dk , WK\\ni ∈ Rdmodel×dk , WV\\ni ∈ Rdmodel×dv\\nand WO ∈ Rhdv×dmodel .  \n",
       "4                                                                                                                                                                                Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [ 1].  \n",
       "5                                                                                            To counteract this effect, we scale the dot products by 1√dk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively.  \n",
       "6                                                                                                                                                                                                 In this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.  \n",
       "0                                                                                                                                                 6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8  \n",
       "1                                                                                                                                                                                                                                                                    The Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.  \n",
       "2                                                    Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers.  \n",
       "3  End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution.  \n",
       "4                                                                                                                                                                                             For example, the largest Transformer explored in\\nVaswani et al. (2017) is (L=6, H=1024, A=16)\\nwith 100M parameters for the encoder, and the\\nlargest Transformer we have found in the literature\\nis (L=64, H=512, A=2) with 235M parameters\\n(Al-Rfou et al., 2018).  \n",
       "5                                                                     7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers.  \n",
       "6                                                                                                                   former is often referred to as a “Transformer encoder” while\\nthe left-context-only version is referred to as a “Transformer\\ndecoder” since it can be used for text generation.\\nIn order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens.  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_dfs = pd.concat([output_df[key] for key in output_df])\n",
    "merged_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4ee12b58-e26e-49dc-ba14-c6be147f5553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['scores', 'text']\n"
     ]
    }
   ],
   "source": [
    "merged_dfs = merged_dfs.reset_index()\n",
    "cols = ['scores','text']\n",
    "print(cols)\n",
    "#cols.remove('index')\n",
    "merged_dfs = merged_dfs.groupby(['question','paper'])[cols].apply(lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d71edef6-f216-4754-b313-7c328188e7f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>scores</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question</th>\n",
       "      <th>paper</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">What is BERT?</th>\n",
       "      <th rowspan=\"7\" valign=\"top\">BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</th>\n",
       "      <th>0</th>\n",
       "      <td>0.811123</td>\n",
       "      <td>BERT is the ﬁrst ﬁne-\\ntuning based representation model that achieves\\nstate-of-the-art performance on a large suite\\nof sentence-level and token-level tasks, outper-\\nforming many task-speciﬁc architectures.\\n• BERT advances the state of the art for eleven\\nNLP tasks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.751915</td>\n",
       "      <td>BERT is conceptually simple and empirically\\npowerful. It obtains new state-of-the-art re-\\nsults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.751781</td>\n",
       "      <td>BERT BERT\\nE[CLS] E1  E[SEP]. EN E1’ . EM’\\nC\\n T1\\n T[SEP].\\n TN\\n T1’ .\\n TM’\\n[CLS] Tok 1  [SEP]. Tok N Tok 1 . TokM\\nQuestion Paragraph\\nStart/End Span\\nBERT\\nE[CLS] E1  E[SEP].</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.740903</td>\n",
       "      <td>TokM\\nQuestion Paragraph\\nStart/End Span\\nBERT\\nE[CLS] E1  E[SEP]. EN E1’ . EM’\\nC\\n T1\\n T[SEP].\\n TN\\n T1’ .\\n TM’\\n[CLS] Tok 1  [SEP]. Tok N Tok 1 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.738021</td>\n",
       "      <td>Input/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g., ⟨Question, Answer ⟩) in one token sequence.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.733836</td>\n",
       "      <td>Each downstream task has sep-\\narate ﬁne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\nA distinctive feature of BERT is its uniﬁed ar-\\nchitecture across different tasks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.731738</td>\n",
       "      <td>BERT\\nE[CLS] E1  E[SEP]. EN E1’ . EM’\\nC\\n T1\\n T[SEP].\\n TN\\n T1’ .\\n TM’\\n[CLS] Tok \\n1\\n [SEP]. Tok \\nN\\nTok \\n1 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">What is Transformer?</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf</th>\n",
       "      <th>18</th>\n",
       "      <td>0.668299</td>\n",
       "      <td>For example, the largest Transformer explored in\\nVaswani et al. (2017) is (L=6, H=1024, A=16)\\nwith 100M parameters for the encoder, and the\\nlargest Transformer we have found in the literature\\nis (L=64, H=512, A=2) with 235M parameters\\n(Al-Rfou et al., 2018).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.657973</td>\n",
       "      <td>former is often referred to as a “Transformer encoder” while\\nthe left-context-only version is referred to as a “Transformer\\ndecoder” since it can be used for text generation.\\nIn order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">attention is all you need 1706.03762v7.pdf</th>\n",
       "      <th>14</th>\n",
       "      <td>0.696613</td>\n",
       "      <td>6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.695771</td>\n",
       "      <td>The Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.679655</td>\n",
       "      <td>Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.674317</td>\n",
       "      <td>End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.662332</td>\n",
       "      <td>7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">What is multi-head attention?</th>\n",
       "      <th rowspan=\"7\" valign=\"top\">attention is all you need 1706.03762v7.pdf</th>\n",
       "      <th>7</th>\n",
       "      <td>0.800710</td>\n",
       "      <td>Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.784158</td>\n",
       "      <td>3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.772050</td>\n",
       "      <td>output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.758912</td>\n",
       "      <td>With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\nwhere headi = Attention(QWQ\\ni , KWK\\ni , V WV\\ni )\\nWhere the projections are parameter matricesWQ\\ni ∈ Rdmodel×dk , WK\\ni ∈ Rdmodel×dk , WV\\ni ∈ Rdmodel×dv\\nand WO ∈ Rhdv×dmodel .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.740496</td>\n",
       "      <td>Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [ 1].</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.738943</td>\n",
       "      <td>To counteract this effect, we scale the dot products by 1√dk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.738749</td>\n",
       "      <td>In this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                     scores  \\\n",
       "question                      paper                                                                                                           \n",
       "What is BERT?                 BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf 0   0.811123   \n",
       "                                                                                                                               1   0.751915   \n",
       "                                                                                                                               2   0.751781   \n",
       "                                                                                                                               3   0.740903   \n",
       "                                                                                                                               4   0.738021   \n",
       "                                                                                                                               5   0.733836   \n",
       "                                                                                                                               6   0.731738   \n",
       "What is Transformer?          BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf 18  0.668299   \n",
       "                                                                                                                               20  0.657973   \n",
       "                              attention is all you need 1706.03762v7.pdf                                                       14  0.696613   \n",
       "                                                                                                                               15  0.695771   \n",
       "                                                                                                                               16  0.679655   \n",
       "                                                                                                                               17  0.674317   \n",
       "                                                                                                                               19  0.662332   \n",
       "What is multi-head attention? attention is all you need 1706.03762v7.pdf                                                       7   0.800710   \n",
       "                                                                                                                               8   0.784158   \n",
       "                                                                                                                               9   0.772050   \n",
       "                                                                                                                               10  0.758912   \n",
       "                                                                                                                               11  0.740496   \n",
       "                                                                                                                               12  0.738943   \n",
       "                                                                                                                               13  0.738749   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 text  \n",
       "question                      paper                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "What is BERT?                 BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf 0                                                                                                                                                                                       BERT is the ﬁrst ﬁne-\\ntuning based representation model that achieves\\nstate-of-the-art performance on a large suite\\nof sentence-level and token-level tasks, outper-\\nforming many task-speciﬁc architectures.\\n• BERT advances the state of the art for eleven\\nNLP tasks.  \n",
       "                                                                                                                               1                                                                                                                                                                    BERT is conceptually simple and empirically\\npowerful. It obtains new state-of-the-art re-\\nsults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.  \n",
       "                                                                                                                               2                                                                                                                                                                                                                                                                                BERT BERT\\nE[CLS] E1  E[SEP]. EN E1’ . EM’\\nC\\n T1\\n T[SEP].\\n TN\\n T1’ .\\n TM’\\n[CLS] Tok 1  [SEP]. Tok N Tok 1 . TokM\\nQuestion Paragraph\\nStart/End Span\\nBERT\\nE[CLS] E1  E[SEP].  \n",
       "                                                                                                                               3                                                                                                                                                                                                                                                                                                              TokM\\nQuestion Paragraph\\nStart/End Span\\nBERT\\nE[CLS] E1  E[SEP]. EN E1’ . EM’\\nC\\n T1\\n T[SEP].\\n TN\\n T1’ .\\n TM’\\n[CLS] Tok 1  [SEP]. Tok N Tok 1 .  \n",
       "                                                                                                                               4                                                                                                                                                                                                                   Input/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g., ⟨Question, Answer ⟩) in one token sequence.  \n",
       "                                                                                                                               5                                                                                                                                                 Each downstream task has sep-\\narate ﬁne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\nA distinctive feature of BERT is its uniﬁed ar-\\nchitecture across different tasks.  \n",
       "                                                                                                                               6                                                                                                                                                                                                                                                                                                                                                BERT\\nE[CLS] E1  E[SEP]. EN E1’ . EM’\\nC\\n T1\\n T[SEP].\\n TN\\n T1’ .\\n TM’\\n[CLS] Tok \\n1\\n [SEP]. Tok \\nN\\nTok \\n1 .  \n",
       "What is Transformer?          BERT pre_training of deep bidirectional transformers for language understanding 1810.04805v2.pdf 18                                                                                                                                                                                             For example, the largest Transformer explored in\\nVaswani et al. (2017) is (L=6, H=1024, A=16)\\nwith 100M parameters for the encoder, and the\\nlargest Transformer we have found in the literature\\nis (L=64, H=512, A=2) with 235M parameters\\n(Al-Rfou et al., 2018).  \n",
       "                                                                                                                               20                                                                                                                   former is often referred to as a “Transformer encoder” while\\nthe left-context-only version is referred to as a “Transformer\\ndecoder” since it can be used for text generation.\\nIn order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens.  \n",
       "                              attention is all you need 1706.03762v7.pdf                                                       14                                                                                                                                                 6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8  \n",
       "                                                                                                                               15                                                                                                                                                                                                                                                                    The Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.  \n",
       "                                                                                                                               16                                                    Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers.  \n",
       "                                                                                                                               17  End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution.  \n",
       "                                                                                                                               19                                                                     7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers.  \n",
       "What is multi-head attention? attention is all you need 1706.03762v7.pdf                                                       7                                                                                                                                Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.  \n",
       "                                                                                                                               8                                                                                     3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence.  \n",
       "                                                                                                                               9                                                                                                                                             output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.  \n",
       "                                                                                                                               10                                                                                                                                                                      With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\nwhere headi = Attention(QWQ\\ni , KWK\\ni , V WV\\ni )\\nWhere the projections are parameter matricesWQ\\ni ∈ Rdmodel×dk , WK\\ni ∈ Rdmodel×dk , WV\\ni ∈ Rdmodel×dv\\nand WO ∈ Rhdv×dmodel .  \n",
       "                                                                                                                               11                                                                                                                                                                                Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [ 1].  \n",
       "                                                                                                                               12                                                                                            To counteract this effect, we scale the dot products by 1√dk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively.  \n",
       "                                                                                                                               13                                                                                                                                                                                                 In this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dc51b8-9d78-4588-a607-cb6a52a33d65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2667ef7d-136a-4c57-93a6-9943e9b788c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b31c68-c701-4f3f-b908-fe8c8b8d15a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5863df96-aa99-4506-b39f-96cf21f85fac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3059c3b-a7bb-4d23-8c52-fdd8dd74a2b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d3311d-25a6-4075-9f55-6f4d597f3fec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df060f60-f574-45a8-8806-1badf01cb53c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
